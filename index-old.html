<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ying Tai(邰颖)</title>
  
  <meta name="author" content="Ying Tai">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

	
<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name><strong><font size="4px">Ying Tai(邰颖)</font></strong></name>
              </p>
	      <p>
	      I will be joining Nanjing University (Suzhou Campus) as an Associate Professor in Aug. 2023. My research interests include Frontier Generative AI research and applications based on advanced large vision and language models. 
	      </p>
              <p>
	      Previously, I was a Principal Researcher and Team Lead at Tencent Youtu Lab, where I spent more than 6 wonderful years, leading a team developing novel vision algorithms that are applied in several products, e.g., Virtual Background feature in Tencent Meeting, High-fidelity face generation APIs in Tencent Cloud and Talking Face Generation for digital human product. Also, our team conducted cutting-edge research works that are published in top-tier AI conferences (NeurIPS/CVPR/ICCV).
	      </p>
	      <p>
              I got my Ph.D. degree from the Department of Computer Science and Engineering, Nanjing University of Science & Technology (NUST) in 2017, and my advisor is Prof. <a href="http://www.patternrecognition.cn/~jian/">Jian Yang</a>. In 2016, I spent 6 wonderful months as a visiting student at Prof. <a href="http://www.cse.msu.edu/~liuxm/">Xiaoming Liu</a>'s lab in Michigan State University. 
              <p style="text-align:center">
                <a href="mailto:tyshiwo@gmail.com">Email</a> (tyshiwo [at] gmail [dot] com) &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=NKaiUasAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/tyshiwo/">Github</a> &nbsp/&nbsp
		<a href="https://www.scopus.com/authid/detail.uri?authorId=56437886200">Scopus</a>
              </p>
            </td>
            
            <td style="padding:2.5%;width:45%;max-width:45%">
              <a href="./projects/dog&me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="./projects/dog&me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	      
	      
<p style="text-align:left"><strong><font size="4px">Recent news</font></strong></p> 
		<ul>
	          <li><p style="text-align:left">03/2023 – 1 paper accepted by ICCV'23 </p></li>
		  <li><p style="text-align:left">05/2023 – I will be an Area Chair for <a href="https://wacv2024.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2024</b></a> </p></li>
		  <li><p style="text-align:left">03/2023 – 3 papers accepted by CVPR'23 </p></li>
		  <li><p style="text-align:left">11/2022 – 2 papers accepted by AAAI'23 (1 Oral and 1 Poster) </p></li>
		  <li><p style="text-align:left">09/2022 – 1 paper accepted by ACM Transactions on Graphics 2022 </p></li>
		  <li><p style="text-align:left">07/2022 – 5 papers accepted by ECCV'22 </p></li>
		  <li><p style="text-align:left">06/2022 – Our CDSR on blind super resolution is accepted by ACM MM'22, with the acceptance rate to be <b>27.9%</b> </p></li>
		  <li><p style="text-align:left">06/2022 – Our AutoGAN-Synthesizer on MRI reconstruction is accepted by MICCAI'22 </p></li>
		  <li><p style="text-align:left">05/2022 – I will be Area Chairs for <a href="https://wacv2023.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2023</b></a> and <a href="http://fg2023.ieee-biometrics.org/organizers/"; style="color: #EE7F2D;"> <b>FG 2023</b></a> </p></li>
		  <li><p style="text-align:left">04/2022 – Our HifiHead on high-fidelity Neural Head Synthesis is accepted by IJCAI'22, with the acceptance rate to be <b>15%</b> </p></li>
		  <li><p style="text-align:left">03/2022 – Our face recognition work <a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"; style="color: #EE7F2D;"> <b>CurricularFace (CVPR'20)</b></a> is inlcuded in <a href="https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf"; style="color: #EE7F2D;"> <b>2022 AI index report from Stanford University</b></a> </p></li>
		  <li><p style="text-align:left">03/2022 – 5 papers accepted by CVPR'22, with the acceptance rate to be <b>25.3%</b> </p></li>
		  <li><p style="text-align:left">03/2022 – First Prize of Progress in Science and Technology of Jiangsu Province (4/11), <a href="http://www.jiangsu.gov.cn/module/download/downfile.jsp?classid=0&filename=d926727e803e49b788d4d7e909a5949e.pdf"; style="color: #EE7F2D;"> <b>“Image restoration and robust recognition: theory and algorithms”</b></a></p></li>
		  <li><p style="text-align:left">02/2022 – I will be an Area Chair for <a href="https://eccv2022.ecva.net/organizers/"; style="color: #EE7F2D;"> <b>ECCV 2022</b></a></p></li>
		  <li><p style="text-align:left">12/2021 – 3 papers accepted by AAAI'22 (1 Oral and 2 Posters), with the acceptance rate to be <b>15%</b> </p></li>
		  <li><p style="text-align:left">09/2021 – 2 papers on blind SR and ViT accepted by NeurIPS'21, with the acceptance rate to be <b>26%</b> </p></li>
		  <li><p style="text-align:left">07/2021 – 2 papers on crowd counting accepted by ICCV'21 (1 Oral and 1 Poster), with the acceptance rate to be <b>25.9%</b> </p></li>
		  <li><p style="text-align:left">07/2021 – Our <a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;"> <b>ASFD</b></a> on face detection is accepted by ACM MM'21 </p></li>
		  <li><p style="text-align:left">04/2021 – 4 papers accepted by IJCAI'21, with the acceptance rate to be <b>13.9%</b> </p></li>
		  <li><p style="text-align:left">04/2021 – Our Team <b>Imagination</b> is the winner of CVPR NTIRE 2021 Challenge on Video Spatial-Temporal Super-Resolution</p></li>
		  <li><p style="text-align:left">03/2021 – 3 papers accepted by CVPR'21 (1 Oral and 2 Posters), with the acceptance rate to be <b>23.7%</b> </p></li>
		  <li><p style="text-align:left">12/2020 – 4 papers accepted by AAAI'21, with the acceptance rate to be <b>21%</b> </p></li>
		  <li><p style="text-align:left">09/2020 – Training codes of RealSR are available in Tencent official github account <a href="https://github.com/Tencent/Real-SR" target="_blank" rel="external">[Tencent-RealSR]</a>. </p></li>
		  <li><p style="text-align:left">07/2020 – 6 papers accepted by ECCV'20, with the acceptance rate to be <b>27%</b> </p></li>
		  <li><p style="text-align:left">05/2020 – Our RealSR model (Team name: Impressionism) won both tracks of <a href="https://arxiv.org/pdf/2005.01996.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a> </p></li>
		  <li><p style="text-align:left">02/2020 – 3 papers accepted by CVPR'20, with the acceptance rate to be <b>22.1%</b> </p></li>
	          <li><p style="text-align:left">11/2019 – 2 papers (Action Proposal & Action Recognition) accepted by AAAI'20, with the acceptance rate to be <b>20.6%</b>. The code of our <a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;"> <b>DBG</b></a> is released at <a href="https://github.com/TencentYoutuResearch/ActionDetection-DBG" target="_blank" rel="external">[ActionDetection-DBG]</a>, which achieves <b>Top 1</b> performance on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a> </p></li>
	          <li><p style="text-align:left">02/2019 – Our <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>DSFD</b></a> on face detection is accepted by CVPR'19, with the acceptance rate to be <b>25.2%</b> </p></li>
                  <li><p style="text-align:left">11/2018 – 2 papers (face alignement & adaptive metric learning) accepted by AAAI'19, with the acceptance rate to be ONLY <b>16.2%</b> </p></li>
                  <li><p style="text-align:left">10/2018 – We released a novel <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>Dual Shot Face Detector (DSFD) </b> </a> framework that achieves <b>Top 1</b> performance on all FIVE settings of <b>WIDER FACE (Easy/Medium/Hard)</b> and <b>FDDB (Discrete/Continuous)</b> datasets </p></li>
	          <li><p style="text-align:left">07/2018 – 1 paper accepted by ECCV'18 </p></li>
	          <li><p style="text-align:left">02/2018 – 1 paper accepted by CVPR'18 （SPOTLIGHT Presentation）</p></li>	
	          <li><p style="text-align:left">07/2017 – 1 paper accepted by ICCV'17 (SPOTLIGHT Presentation) </p></li>
	          <li><p style="text-align:left">03/2017 – 1 paper accepted by CVPR'17 </p></li>
	        </ul>
	 
	<br> <a name="Pub"></a>
	<p style="text-align:justify"><strong><font size="4px">Preprints</font></strong> (* equal contribution, <sup>#</sup> corresponding author)
		<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
			
	   <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/CtlGAN_arXiv2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2203.08612.pdf">
              <papertitle><b>CtlGAN: Few-shot Artistic Portraits Generation with Contrastive Transfer Learning</b></papertitle>
              </a>
              <br>
              Y. Wang, R. Yi, <b>Y. Tai</b>, C. Wang, and L. Ma.              
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.08612.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p></p>
            </td><tr> 
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/CFNet-arXiv22.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2201.04796.pdf">
              <papertitle><b>CFNet: Learning Correlation Functions for One-Stage Panoptic Segmentation</b></papertitle>
              </a>
              <br>
              Y. Chen*, W. Chu*, F. Wang, <b>Y. Tai</b>, R. Yi, Z. Gan, L. Yao, C. Wang and X. Li.              
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2201.04796.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p></p>
            </td><tr>
			
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/Arxiv20-Style.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://static.aminer.cn/storage/pdf/arxiv/20/2007/2007.01758.pdf">
              <papertitle><b>Collaborative Learning for Faster StyleGAN Embedding</b></papertitle>
              </a>
              <br>
              S. Guan, <b>Y. Tai</b>, B. Ni, F. Zhu, F. Huang and X. Yang.              
              <br>
              <em>arXiv</em>, 2020 
              <br>
              <a href="https://static.aminer.cn/storage/pdf/arxiv/20/2007/2007.01758.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p></p>
            </td><tr>


            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/AG_arxiv19.JPG" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.00713">
              <papertitle><b>Aurora Guard: Reliable Face Anti-Spoofing via Mobile Lighting System</b></papertitle>
              </a>
              <br>
              J. Zhang, <b>Y. Tai</b>, T. Yao, J. Meng, S. Ding, C. Wang, J. Li, F. Huang and R. Ji.             
              <br>
              <em>arXiv</em>, 2021 
              <br>
              <a href="https://arxiv.org/abs/2102.00713"; style="color: #EE7F2D;">arXiv</a>
            
            </td><tr>
			
        </table></p>
 
 <p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <sup>#</sup> corresponding author)
		<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">

	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/ImAR-arXIv2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2203.08612.pdf">
              <papertitle><b>Learning Versatile 3D Shape Generation with Improved AR Models</b></papertitle>
              </a>
              <br>
              S. Luo, X. Qian, Y. Fu, Y. Zhang, <b>Y. Tai</b>, Z. Zhang, C. Wang, and X. Xue.              
              <br>
              <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2303.14700.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p></p>
            </td><tr> 

		    
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/NPF-CVPR2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1r8jcuDpYAnwRM9HOaVJr9BgvRf9L0Vks/view?usp=share_link">
              <papertitle><b>Learning Neural Proto-face Field for Disentangled 3D Face Modeling In the Wild</b></papertitle>
              </a>
              <br>
	      Z. Zhang, R. Chen, W. Cao, <b>Y. Tai</b><sup>#</sup>, and C. Wang<sup>#</sup>
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
              <br>
	      <a href="https://drive.google.com/file/d/1r8jcuDpYAnwRM9HOaVJr9BgvRf9L0Vks/view?usp=share_link"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	    
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/CALoss-CVPR2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1mKZ8e78DtXATv8_oZaB4d-LMBrc6UeEb/view?usp=share_link">
              <papertitle><b>Learning to Measure the Point Cloud Reconstruction Loss in a Representation Space</b></papertitle>
              </a>
              <br>
	      T. Huang, Z. Ding, J. Zhang, <b>Y. Tai</b>, Z. Zhang, M. Chen, C. Wang, and Y. Liu
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
              <br>
	      <a href="https://drive.google.com/file/d/1mKZ8e78DtXATv8_oZaB4d-LMBrc6UeEb/view?usp=share_link"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/HiTalk-CVPR2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2305.02572.pdf">
              <papertitle><b>High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning</b></papertitle>
              </a>
              <br>
	      C. Xu, J. Zhang, J. Zhu, W. Chu, <b>Y. Tai</b>, C. Wang, and Y. Liu
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
              <br>
	      <a href="https://arxiv.org/pdf/2305.02572.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/HitNet_AAAI2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2203.11624.pdf">
              <papertitle><b>High-resolution Iterative Feedback Network for Camouflaged Object Detection</b></papertitle>
              </a>
              <br>
	      X. Hu, S. Wang, X. Qian, H. Dai, W. Ren, D. Luo, <b>Y. Tai</b>, and L. Shao
              <br>
              <em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2023
              <br>
	      <a href="https://arxiv.org/pdf/2203.11624.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
            </td><tr>
          
	
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/CRI_AAAI2023.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2302.03406.pdf">
              <papertitle><b>High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets</b></papertitle>
              </a>
              <br>
	      Y. Wang, C. Lin, D. Luo, <b>Y. Tai</b>, Z. Zhang and Y. Xie
              <br>
              <em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2023 <b style="color: red">[Oral]</b> 
              <br>
	      <a href="https://arxiv.org/pdf/2302.03406.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
            </td><tr>		

	
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/3QNet_TOG2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1Sl0ttF1I4uuH7WpZ89hqFliIrSRnkcYT/view?usp=sharing">
              <papertitle><b>3QNet: 3D Point Cloud Geometry Quantization Compression Network</b></papertitle>
              </a>
              <br>
	      T. Huang, J. Zhang, J. Chen, Z. Ding, <b>Y. Tai</b>, Z. Zhang, C. Wang, and Y. Liu
              <br>
              <em>ACM Transactions on Graphics</em>, 2022
              <br>
	      <a href="https://drive.google.com/file/d/1Sl0ttF1I4uuH7WpZ89hqFliIrSRnkcYT/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/ColorFormer_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/19jhYLzOsCpDsG1ntvg6rfV-4NzHPKEwW/view?usp=sharing">
              <papertitle><b>ColorFormer: Image Colorization via Color Memory assisted Hybrid-attention Transformer</b></papertitle>
              </a>
              <br>
	      X. Ji*, B. Jiang*, D. Luo, G. Tao, W. Chu, <b>Y. Tai</b><sup>#</sup>, Z. Xie, and C. Wang<sup>#</sup>
              <br>
              <em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
              <br>
	      <a href="https://drive.google.com/file/d/19jhYLzOsCpDsG1ntvg6rfV-4NzHPKEwW/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
		    /
	      <a href="https://drive.google.com/file/d/1-hhTHW9aQ60JJqYKIs-6tEB1xtesjG60/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/StyleFace_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1E2VzLtjWAM9I4ZkvveSLMqXz-frf707C/view?usp=sharing">
              <papertitle><b>StyleFace: Towards Identity-Disentangled Face Generation on Megapixels</b></papertitle>
              </a>
              <br>
	      Y. Luo, J. Yan, J. Zhu, K. He, W. Chu,  <b>Y. Tai</b>, and C. Wang
              <br>
              <em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
              <br>
	      <a href="https://drive.google.com/file/d/1E2VzLtjWAM9I4ZkvveSLMqXz-frf707C/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
		   /
	      <a href="https://drive.google.com/file/d/1ncWoQXQRa7Sg_dKSpbycX0x2ieHlHZJB/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>  
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/FRS_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="google.com">
              <papertitle><b>Designing One Unified Framework for High-Fidelity Face Reenactment and Swapping</b></papertitle>
              </a>
              <br>
	      C. Xu*, J.  Zhang*, Y. Han, G. Tian, X. Zeng, <b>Y. Tai</b>, Y. Wang, C. Wang, and Y. Liu
              <br>
              <em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
              <br>
	      <a href="https://drive.google.com/file/d/1uahR40VteHhBdjRVEN_hPwN9g5r54GZ1/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
		    /
	      <a href="https://drive.google.com/file/d/1CDDtFN6txDdALMkq0Qpc2lFGstmtnntv/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
		    /
	      <a href="https://github.com/xc-csc101/UniFace"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/xc-csc101/UniFace.svg" alt="GitHub stars" title="" />
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/SeedFormer_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2207.10315.pdf">
              <papertitle><b>SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer</b></papertitle>
              </a>
              <br>
	      H. Zhou, Y. Cao, W. Chu, J. Zhu, L. Tong, <b>Y. Tai</b>, and C. Wang
              <br>
              <em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
              <br>
	      <a href="https://arxiv.org/pdf/2207.10315.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
		    /
	      <a href="https://drive.google.com/file/d/1p2ubWO25o9e0d0hez6hmzXahww5j_dQc/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
		    /
	      <a href="https://github.com/hrzhou2/seedformer"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/hrzhou2/seedformer.svg" alt="GitHub stars" title="" />
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/ProCA_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2207.06654.pdf">
              <papertitle><b>Prototypical Contrast Adaptation for Domain Adaptive Semantic Segmentation</b></papertitle>
              </a>
              <br>
	      Z. Jiang, Y. Li, C. Yang, P. Gao, Y. Wang, <b>Y. Tai</b>, and C. Wang
              <br>
              <em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
              <br>
	      <a href="https://arxiv.org/pdf/2207.06654.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
		    /
	      <a href="https://github.com/jiangzhengkai/ProCA"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/jiangzhengkai/ProCA.svg" alt="GitHub stars" title="" />
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/CDSR_MM22.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2208.13436.pdf">
              <papertitle><b>Joint Learning Content and Degradation Aware Feature for Blind Super-Resolution</b></papertitle>
              </a>
              <br>
	      Y. Zhou*, C. Lin*, D. Luo, Y. Liu, Mingang Chen, <b>Y. Tai</b><sup>#</sup>, and C. Wang<sup>#</sup>
              <br>
              <em>ACM International Conference on Multimedia (<b>ACM MM</b>)</em>, 2022
              <br>
	      <a href="https://arxiv.org/pdf/2208.13436.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
            </td><tr>
	
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/AutoGAN-Synthesizer_MICCAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-031-16446-0_38">
              <papertitle><b>AutoGAN-Synthesizer: Neural Architecture Searchfor Cross-Modality MRI Synthesis</b></papertitle>
              </a>
              <br>
	      X. Hu, R. Shen, D. Luo, <b>Y. Tai</b>, C. Wang, and B. Menze
              <br>
              <em>International Conference on Medical Image Computing and Computer Assisted Intervention (<b>MICCAI</b>)</em>, 2022
              <br>
	      <a href="https://link.springer.com/chapter/10.1007/978-3-031-16446-0_38"; style="color: #EE7F2D;">Paper (official link)</a>
	      /
	      <a href="https://github.com/HUuxiaobin/AutoGAN-Synthesizer"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/HUuxiaobin/AutoGAN-Synthesizer.svg" alt="GitHub stars" title="" />
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/HifiHead_IJCAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1ZLG3hQuXJ9IkOqvhKyMHuPvS55sJXud3/view?usp=sharing">
              <papertitle><b>HifiHead: One-Shot High Fidelity Neural Head Synthesis with 3D Control</b></papertitle>
              </a>
              <br>
              F. Zhu, J. Zhu, W. Chu, <b>Y. Tai</b><sup>#</sup>, Z. Xie, X. Huang, and C. Wang<sup>#</sup>.        
              <br>
              <em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2022
              <br>
	      <a href="https://drive.google.com/file/d/1ZLG3hQuXJ9IkOqvhKyMHuPvS55sJXud3/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/SGPN_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/18f2Spu3Qx1gskjdGh1Edx1ZlWjlC8xGB/view?usp=sharing">
              <papertitle><b>Blind Face Restoration via Integrating Face Shape and Generative Priors</b></papertitle>
              </a>
              <br>
              F. Zhu, J. Zhu, W. Chu, X. Zhang, X. Ji, C. Wang<sup>#</sup>, and <b>Y. Tai</b><sup>#</sup>.        
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/18f2Spu3Qx1gskjdGh1Edx1ZlWjlC8xGB/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
	       /
	      <a href="https://drive.google.com/file/d/1BhJa5I95sAzENzo0_He-L8vcsYFlKL9N/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
	       /
	      <a href="https://github.com/TencentYoutuResearch/FaceRestoration-sgpn"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/FaceRestoration-sgpn.svg" alt="GitHub stars" title="" />
            </td><tr>
	
	 <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/IFRNet_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1oj4uCqu7E2913JKLohu7nZsj5SvECCdq/view?usp=sharing">
              <papertitle><b>IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation</b></papertitle>
              </a>
              <br>
              L. Kong*, B. Jiang*, D. Luo, W. Chu, X. Huang, <b>Y. Tai</b>, C. Wang, and J. Yang.        
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/1oj4uCqu7E2913JKLohu7nZsj5SvECCdq/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
	      /
	      <a href="https://drive.google.com/file/d/15K_m9HhGQ4MfrUREr0DxcdtEuCnv_OD3/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>    
	      /
	      <a href="https://github.com/ltkong218/IFRNet"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ltkong218/IFRNet.svg" alt="GitHub stars" title="" />
            </td><tr>
	
	<td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/phyDIR_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1s-6glim2XEKNGmrRJKrXyKFJ1X58B7GA/view?usp=sharing">
              <papertitle><b>Physically-guided Disentangled Implicit Rendering for 3D Face Modeling</b></papertitle>
              </a>
              <br>
              Z. Zhang, Y. Ge, <b>Y. Tai</b>, W. Cao, R. Chen, K. Liu, H. Tang, X. Huang, C. Wang, Z. Xie, and D. Huang.        
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/1s-6glim2XEKNGmrRJKrXyKFJ1X58B7GA/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
	      /
	      <a href="https://drive.google.com/file/d/1UohmPBmgoo6-Dh7ES2ugLvy9-X2TVDNi/view?usp=sharing"; style="color: #EE7F2D;">Supp</a> 
            </td><tr>
	
	<td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/RDF_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/14A4zxu6u5vQ9kROA4HvWllRQ72P8Ml6A/view?usp=sharing">
              <papertitle><b>Learning to Restore 3D Face from In-the-Wild Degraded Images</b></papertitle>
              </a>
              <br>
              Z. Zhang, Y. Ge, <b>Y. Tai</b>, X. Huang, C. Wang, H. Tang, D. Huang, and Z. Xie.        
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/14A4zxu6u5vQ9kROA4HvWllRQ72P8Ml6A/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
              /
	      <a href="https://drive.google.com/file/d/1ahwfZtPB4YrLWXswsDz2jSPGon2ZCiZa/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
            </td><tr>
	
	<td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/MFH_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1Gq-167f2ue30463K4XMkHOJThg9Yugk0/view?usp=sharing">
              <papertitle><b>Learning to Memorize Feature Hallucination for One-Shot Image Generation</b></papertitle>
              </a>
              <br>
              Y. Xie, Y. Fu, J. Zhu, <b>Y. Tai</b>, Y. Cao, and C. Wang.        
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/1Gq-167f2ue30463K4XMkHOJThg9Yugk0/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	  <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/DIRL-AAAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1dlX3bHVJ8-djyDeuK3B-EhJoNEwxQMSY/view?usp=sharing">
              <papertitle><b>DIRL: Domain-invariant Representation Learning for Generalizable Semantic Segmentation</b></papertitle>
              </a>
              <br>
              Q. Xu, L. Yao, Z. Jiang, G. Jiang, W. Chu, W. Han, W. Zhang, C. Wang, and <b>Y. Tai</b><sup>#</sup>.        
              <br>
              <em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022 <b style="color: red">[Oral]</b> 
              <br>
              <a href="https://drive.google.com/file/d/1dlX3bHVJ8-djyDeuK3B-EhJoNEwxQMSY/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
	
	<td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/SCSNet-AAAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1LOo18uoqCBZ_fb3HJYOcBVmUMBEVCZRj/view?usp=sharing">
              <papertitle><b>SCSNet: Simultaneously Image Colorization and Super-Resolution</b></papertitle>
              </a>
              <br>
              J. Zhang, C. Xu, Y. Han, J. Li, Y. Wang, <b>Y. Tai</b>, C. Wang, F. Huang, Z. Xie, and Y. Liu.      
              <br>
              <em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/1LOo18uoqCBZ_fb3HJYOcBVmUMBEVCZRj/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	<td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/LCTR-AAAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1aCz2WOuxa3ppYkb2i43i6_WamA9drLU9/view?usp=sharing">
              <papertitle><b>LCTR: On Awakening the Local Continuity of Transformer for Weakly Supervised Object Localization</b></papertitle>
              </a>
              <br>
              Z. Chen, C. Wang, Y. Wang, G. Jiang, Y. Shen, <b>Y. Tai</b>, C. Wang, W. Zhang, and L. Cao.       
              <br>
              <em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022
              <br>
              <a href="https://drive.google.com/file/d/1aCz2WOuxa3ppYkb2i43i6_WamA9drLU9/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
            </td><tr>
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/S2K-nips2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2110.12151.pdf">
              <papertitle><b>Spectrum-to-Kernel Translation for Accurate Blind Image Super-Resolution</b></papertitle>
              </a>
              <br>
              G. Tao, X. Ji, W. Wang, S. Chen, C. Lin, Y. Cao, T. Lu, D. Luo, and <b>Y. Tai</b>.             
              <br>
              <em>Thirty-fifth Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2110.12151.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p><font color="red">A novel framework S2K that predicts the kernel from spectrum in frequency domain</p>
            </td><tr>
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/EAT-arXiv2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2105.15089.pdf">
              <papertitle><b>Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model</b></papertitle>
              </a>
              <br>
              J. Zhang, C. Xu, J. Li, W. Chen, Y. Wang, <b>Y. Tai</b>, S. Chen, C. Wang, F. Huang and R. Liu.             
              <br>
              <em>Thirty-fifth Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2105.15089.pdf"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://github.com/TencentYoutuResearch/BaseArchitecture-EAT"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/BaseArchitecture-EAT.svg" alt="GitHub stars" title="" />
              <p><font color="red">An interesting viewpoint between EA and Transfomer, and propose an EA based Transformer framework for both NLP and CV tasks</p>
            </td><tr>
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/P2PNet-ICCV2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2107.12746.pdf">
              <papertitle><b>Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework</b></papertitle>
              </a>
              <br>
              Q. Song*, C. Wang*, Z. Jiang, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Wu.              
              <br>
              <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2021 <b style="color: red">[Oral]</b> 
              <br>
              <a href="https://arxiv.org/pdf/2107.12746.pdf"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-P2PNet.svg" alt="GitHub stars" title="" />
              <p><font color="red">We propose a novel simple and elegant framework for crowd counting, which directly predicts the crowd location instead of using density map estimation adopted in most previous methods.</font></p>
            </td><tr>
			
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/UEPNet-ICCV2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2107.12619.pdf">
              <papertitle><b>Uniformity in Heterogeneity: Diving Deep into Count Interval Partition for Crowd Counting</b></papertitle>
              </a>
              <br>
              C. Wang*, Q. Song*, B. Zhang, Y. Wang, <b>Y. Tai</b>, X. Hu, C. Wang, J. Li, J. Ma, and Y. Wu.             
              <br>
              <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2021 
              <br>
	      <a href="https://arxiv.org/pdf/2107.12619.pdf"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet"; style="color: #EE7F2D;">Code</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-UEPNet.svg" alt="GitHub stars" title="" />
            </td><tr>
	
	
	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/ASFD_arXiv20.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2003.11228.pdf">
              <papertitle><b>ASFD: Automatic and Scalable Face Detector</b></papertitle>
              </a>
              <br>
              J. Li*, B. Zhang*, Y. Wang, <b>Y. Tai</b>, Z. Zhang, C. Wang, J. Li, X. Huang and Y. Xia.              
              <br>
              <em>ACM International Conference on Multimedia (<b>ACM MM</b>)</em>, 2021 
              <br>
              <a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;">arXiv</a>
              <p><font color="red">Ranked No. 1 on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a></font></p>
            </td><tr>
			
			
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/HifiFace-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2106.09965.pdf">
              <papertitle><b>HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping</b></papertitle>
              </a>
              <br>
              Y. Wang*, X. Chen*, J. Zhu, W. Chu, <b>Y. Tai</b><sup>#</sup>, C. Wang, J. Li, Y. Wu, F. Huang and R. Ji.              
              <br>
              <em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2106.09965.pdf"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://johann.wang/HifiFace/"; style="color: #EE7F2D;">Project</a>
	      /
	      <a href="https://drive.google.com/file/d/1K9QsPX2Yw7iUtH33aoP-dTlQhnZLQuWC/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>	
	      /
	      <a href="https://drive.google.com/file/d/19i-4tJD7NxrqtmlRUl9l3M-FuMy7F2EG/view?usp=sharing"; style="color: #EE7F2D;">Video (1min)</a>	
              </p>
            </td><tr>
	
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/SPL-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2106.07220.pdf">
              <papertitle><b>Context-Aware Image Inpainting with Learned Semantic Priors</b></papertitle>
              </a>
              <br>
              W. Zhang, J. Zhu, <b>Y. Tai</b>, Y. Wang, W. Chu, B. Ni, C. Wang and X. Yang.              
              <br>
              <em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2106.07220.pdf"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://arxiv.org/pdf/2112.04107.pdf"; style="color: #EE7F2D;">Extended journal version</a>
	      /
	      <a href="https://github.com/WendongZh/SPL"; style="color: #EE7F2D;">Code (Official)</a>    
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/WendongZh/SPL.svg" alt="GitHub stars" title="" />
              </p>
            </td><tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/DRDG-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2106.16128.pdf">
              <papertitle><b>Dual Reweighting Domain Generalization for Face Presentation Attack Detection</b></papertitle>
              </a>
              <br>
              S. Liu, K. Zhang, T. Yao, K. Sheng, S. Ding, <b>Y. Tai</b>, J. Li, Y. Xie and L. Ma.              
              <br>
              <em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2106.16128.pdf"; style="color: #EE7F2D;">arXiv</a>
              </p>
            </td><tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/SiamRCR-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2105.11237.pdf">
              <papertitle><b>SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking</b></papertitle>
              </a>
              <br>
              J. Peng*, Z. Jiang*, Y. Gu*, Y. Wu, Y. Wang, <b>Y. Tai</b>, C. Wang and W. Lin.              
              <br>
              <em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2105.11237.pdf"; style="color: #EE7F2D;">arXiv</a>
              </p>
            </td><tr>
		    
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/3DFace_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Aggregate_and_Personalize_3D_Face_From_In-the-Wild_Photo_CVPR_2021_paper.pdf">
              <papertitle><b>Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection</b></papertitle>
              </a>
              <br>
              Z. Zhang, Y. Ge, R. Chen, <b>Y. Tai</b>, Y. Yan, J. Yang, C. Wang, J. Li, and F. Huang.              
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 <b style="color: red">[Oral]</b> 
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Aggregate_and_Personalize_3D_Face_From_In-the-Wild_Photo_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
	      /
	      <a href="https://github.com/TencentYoutuResearch/3DFaceReconstruction-LAP"; style="color: #EE7F2D;">Code (Official)</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/3DFaceReconstruction-LAP.svg" alt="GitHub stars" title="" />
              </p>
            </td><tr>		
	
		           
            <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./projects/AFSD_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2103.13137">
              <papertitle><b>Learning Salient Boundary Feature for Anchor-free Temporal Action Localization</b></papertitle>
              </a>
              <br>
              C. Lin*, C. Xu*, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.              
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 
              <br>
              <a href="https://arxiv.org/abs/2103.13137"; style="color: #EE7F2D;">arXiv</a>
	      /
	      <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Learning_Salient_Boundary_Feature_for_Anchor-free_Temporal_Action_Localization_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
	      /
	      <a href="https://github.com/TencentYoutuResearch/ActionDetection-AFSD"; style="color: #EE7F2D;">Code (Official)</a>
	      <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/ActionDetection-AFSD.svg" alt="GitHub stars" title="" />
              </p>
            </td><tr>	
            
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/REVIDE_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Restore_Hazy_Video_A_New_Real-World_Dataset_and_CVPR_2021_paper.pdf"><papertitle><b>Learning to Restore Hazy Video: A New Real-World Dataset and A New Method</b></papertitle></a><br>
		X. Zhang*, H. Dong*, J. Pan, C. Zhu, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and F. Wang.
		<br>
                <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 
		<br>
		<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Restore_Hazy_Video_A_New_Real-World_Dataset_and_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
		
		</td></tr>

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/FCA_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2012.10102.pdf"><papertitle><b>Frequency Consistent Adaptation for Real World Super Resolution</b></papertitle></a><br>
		X. Ji*, G. Tao*, Y. Cao, <b>Y. Tai</b>, T. Lu, C. Wang, J. Li, and F. Huang.
		<br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021 
		<br>
		<a href="https://arxiv.org/pdf/2012.10102.pdf"; style="color: #EE7F2D;">arXiv</a>
		</p>
		<p><font color="red">Improved version of our prior work RealSR</font></p>
		</td></tr>			  
            
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CMR_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"><papertitle><b>Learning Comprehensive Motion Representation for Action Recognition</b></papertitle></a><br>
		M. Wu*, B. Jiang*, D. Luo, J. Yan, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, and X. Yang.
		<br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
		<br>
		<a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"; style="color: #EE7F2D;">arXiv</a>
	        /
	        <a href="https://github.com/TencentYoutuResearch/ActionRecognition-CMR"; style="color: #EE7F2D;">Code (Official)</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/ActionRecognition-CMR.svg" alt="GitHub stars" title="" />
		</p>
		<p><font color="red">Extented version of our prior works TEINet and TDRL</font></p>
		</td></tr>	


          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/D2AM_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2105.02453.pdf"><papertitle><b>Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing</b></papertitle></a><br>
		Z. Chen, T. Yao, K. Sheng, S. Ding, <b>Y. Tai</b>, J. Li, F. Huang, and X. Jin.
		<br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
		<br>
		<a href="https://arxiv.org/pdf/2105.02453.pdf"; style="color: #EE7F2D;">arXiv</a>
		</p>
		</td></tr>	

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/SASNet_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"><papertitle><b>To Choose or to Fuse? Scale Selection for Crowd Counting</b></papertitle></a><br>
		Q. Song*, C. Wang*, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, J. Wu, and J. Ma.
		<br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
		<br>
		<a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
	        /
		<a href="https://github.com/TencentYoutuResearch/CrowdCounting-SASNet"; style="color: #EE7F2D;">Code (Official)</a> 
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-SASNet" alt="GitHub stars" title="" />
		</p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/FAN_ArXiv19.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1911.11680.pdf"><papertitle><b>FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization</b></papertitle></a><br>
		X. Yin, <b>Y. Tai</b>, Y. Huang and X. Liu.
		<br>
        <em>Asian Conference on Computer Vision (<b>ACCV</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/1911.11680.pdf"; style="color: #EE7F2D;">Paper</a>
		</p>
		<p><font color="red">Novel framework to improve surveillance face recognition & normalization from unpaired data</font></p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DDL_Arxiv2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2002.03662.pdf"><papertitle><b>Improving Face Recognition from Hard Samples via Distribution Distillation Loss</b></papertitle></a><br>
		Y. Huang*, P. Shen*, <b>Y. Tai</b><sup>#</sup>, S. Li<sup>#</sup>, X. Liu, J. Li, F. Huang, and R. Ji.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/2002.03662.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750137.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/Tencent/TFace"; style="color: #EE7F2D;">Code (Official)</a> 
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/TFace.svg" alt="GitHub stars" title="" />
		</p>
		</td></tr>				
            
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/SSCGAN_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"><papertitle><b>SSCGAN: Facial Attribute Editing via Style Skip Connections</b></papertitle></a><br>
		W. Chu, <b>Y. Tai</b><sup>#</sup>, C. Wang, J. Li, F. Huang, and R. Ji.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		</p>
		</td></tr>		           
            

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DRL_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2008.08250.pdf"><papertitle><b>Face Anti-Spoofing via Disentangled Representation Learning</b></papertitle></a><br>
		K. Zhang, T. Yao, J. Zhang, <b>Y. Tai</b><sup>#</sup>, S. Ding, J. Li, F. Huang, H. Song and L. Ma.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640630.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		</p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CTracker_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2007.14557.pdf"><papertitle><b>Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End <br> Joint Multiple-Object Detection and Tracking</b></papertitle></a><br>
		J. Peng, C. Wang, F. Wan, Y. Wu, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020 <b style="color: red">[Spotlight]</b> 
		<br>
		<a href="https://arxiv.org/pdf/2007.14557.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490137.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/pjl1995/CTracker"; style="color: #EE7F2D;">Code (Official)</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/pjl1995/CTracker.svg" alt="GitHub stars" title="" />
		
	    </p>
		</td></tr>			

          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/TDRL_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2007.07626.pdf"><papertitle><b>Temporal Distinct Representation Learning for 2D-CNN-based Action Recognition</b></papertitle></a><br>
		J. Weng, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, X. Jiang and J. Yuan.		    <br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520358.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
		</p>
		</td></tr>	
		
          <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/HPE_arXiv2019.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2008.00697.pdf"><papertitle><b>Adversarial Semantic Data Augmentation for Human Pose Estimation</b></papertitle></a><br>
		Y. Bin, X. Cao, X. Chen, Y. Ge, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, C. Gao and N. Sang.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/2008.00697.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640596.pdf"; style="color: #EE7F2D;">Paper (Official)</a>	
		/
		<a href="https://github.com/Binyr/ASDA"; style="color: #EE7F2D;">Code (Official)</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Binyr/ASDA.svg" alt="GitHub stars" title="" />
		
	    </p>
	    <p><font color="red">State-of-the-art performance on <a href="http://human-pose.mpi-inf.mpg.de/#results"; style="color: #EE7F2D;"> <b>MPII</b></a></font> and <a href="http://sam.johnson.io/research/lsp.html"; style="color: #EE7F2D;"> <b>LSP</b></a></font></p>
		</td></tr>				
		

         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/RealSR_CVPRW2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2005.01996.pdf"><papertitle><b>Real-World Super-Resolution via Kernel Estimation and Noise Injection</b></papertitle></a><br>
		X. Ji, Y. Cao, <b>Y. Tai</b><sup>#</sup>, C. Wang, J. Li, and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition Workshop (<b>CVPRW</b>)</em>, 2020
		<br>
		<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://github.com/Tencent/Real-SR"; style="color: #EE7F2D;">Code (Tencent)</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/Real-SR.svg" alt="GitHub stars" title="" />
		/
		<a href="https://github.com/Tencent/Real-SR"; style="color: #EE7F2D;">Code (Personal)</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/jixiaozhong/RealSR.svg" alt="GitHub stars" title="" />
	        /
		<a href="https://github.com/nihui/realsr-ncnn-vulkan"; style="color: #EE7F2D;">Code (NCNN-vulkan)</a>
                <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/nihui/realsr-ncnn-vulkan.svg" alt="GitHub stars" title="" />
		/
		<a href="https://openbenchmarking.org/test/pts/realsr-ncnn"; style="color: #EE7F2D;">OpenBenchmarking.org</a>
		/
		<a href="https://arxiv.org/pdf/2005.01996.pdf"; style="color: #EE7F2D;">Challenge Report</a>
			
	    </p>
	    <p><font color="red">Winner of <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a></font></p>
		</td></tr>				
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CurricularFace_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"><papertitle><b>CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition</b></papertitle></a><br>
		Y. Huang, Y. Wang, <b>Y. Tai</b><sup>#</sup>, X. Liu, P. Shen, S. Li<sup>#</sup>, J. Li, and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
		<br>
		<a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/HuangYG123/CurricularFace"; style="color: #EE7F2D;">Code</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/HuangYG123/CurricularFace.svg" alt="GitHub stars" title="" />
		
	    </p>
		</td></tr>			
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/UnsupervisedOpticalFlow_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"><papertitle><b>Learning by Analogy: Reliable Supervision from Transformations for <br> Unsupervised Optical Flow Estimation</b></papertitle></a><br>
		L. Liu, J. Zhang, Y. Liu, Y. Wang, <b>Y. Tai</b>, D. Luo, C. Wang, J. Li, and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
		<br>
		<a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/lliuz/ARFlow"; style="color: #EE7F2D;">Code</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/lliuz/ARFlow.svg" alt="GitHub stars" title="" />
		
	    </p>
		</td></tr>	
	
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/VideoReID_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"><papertitle><b>Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification</b></papertitle></a><br>
		Y. Yan, J. Qin, J. Chen, L. Liu, F. Zhu, <b>Y. Tai</b>, and L. Shao.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
		<br>
		<a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/daodaofr/hypergraph_reid"; style="color: #EE7F2D;">Code</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/daodaofr/hypergraph_reid.svg" alt="GitHub stars" title="" />
		
	    </p>
		</td></tr>	
		
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DBG_AAAI20.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1911.04127.pdf"><papertitle><b>Fast Learning of Temporal Action Proposal via Dense Boundary Generator</b></papertitle></a><br>
		C. Lin*, J. Li*, Y. Wang, <b>Y. Tai</b>, D. Luo, Z. Cui, C. Wang, J. Li, F. Huang and R. Ji.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://github.com/Tencent/ActionDetection-DBG"; style="color: #EE7F2D;">Code</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/ActionDetection-DBG.svg" alt="GitHub stars" title="" />
			  
	    <p><font color="red">Ranked No. 1 on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a></font></p> 
	    </p>
		</td></tr>					
		
         <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/TEI_AAAI20.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1911.09435.pdf"><papertitle><b>TEINet: Towards an Efficient Architecture for Video Recognition</b></papertitle></a><br>
		Z. Liu*, D. Luo*, Y. Wang, L. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and T. Lu.
		<br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2020
		<br>
		<a href="https://arxiv.org/pdf/1911.09435.pdf"; style="color: #EE7F2D;">arXiv</a>
	    <br>	    
	    </p>
		</td></tr>	
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/face_detection_arxiv18.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://128.84.21.199/abs/1810.10220"><papertitle><b>DSFD: Dual Shot Face Detector</b></papertitle></a><br>
		J. Li, Y. Wang, C. Wang, <b>Y. Tai</b>, J. Qian, J. Yang, C.e Wang, J. Li and F. Huang.
		<br>
        <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2019
		<br>
		<a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://drive.google.com/open?id=1DuhcD4Thwv8E3kcLZIOQTeo8qaGDq9PB"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://github.com/Tencent/FaceDetection-DSFD"; style="color: #EE7F2D;">Code</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/FaceDetection-DSFD.svg" alt="GitHub stars" title="" />
		
	    <p><font color="red">Ranked No. 1 on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a> and <a href="http://vis-www.cs.umass.edu/fddb/results.html#eval"; style="color: #EE7F2D;"> <b>FDDB</b></a> (Until 2019.01)</font></p> 
	    </p>
		</td></tr>
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/aaai19_FHR.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1811.00342.pdf"><papertitle><b>Towards Highly Accurate and Stable Face Alignment for High-Resolution Videos</b></papertitle></a><br>
		<b>Y. Tai*</b>, Y. Liang*, X. Liu, L. Duan, J. Li, C. Wang, F. Huang and Y. Chen.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2019
		<br>
		<a href="https://arxiv.org/pdf/1811.00342.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://drive.google.com/open?id=1bDxP_i2ETfoO4uAbPhukCcxvOrVQuwd-"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://drive.google.com/open?id=1cFyjZWdGOBZ8t-63bZehERMaKpTkawwe"; style="color: #EE7F2D;">Supp</a>
		/
		<a href="https://github.com/tyshiwo/FHR_alignment"; style="color: #EE7F2D;">Code</a>
	        <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/FHR_alignment.svg" alt="GitHub stars" title="" />
		
	    </p>
		</td></tr>		
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/DAML_aaai19.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"><papertitle><b>Data-Adaptive Metric Learning with Scale Alignment</b></papertitle></a><br>
		S. Chen, C. Gong, J. Yang, <b>Y. Tai</b>, L. Hui and J. Li.
		<br>
        <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2019
		<br>
		<a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"; style="color: #EE7F2D;">Paper (Official)</a>
		<br>
	    </p>
		</td></tr>						


		<td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/CD_ECCV18.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"><papertitle><b>Person Search via A Mask-Guided Two-Stream CNN Model</b></papertitle></a><br>
		D. Chen, S. Zhang, W. Ouyang, J. Yang and <b>Y. Tai</b>.
		<br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2018
		<br>
		<a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://drive.google.com/file/d/1Wg4ieK4BQc5XMZCBPpQJXumoHs21AHBM/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
		<br>
	    </p>
		</td></tr>	
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		  <a href="./projects/faceSR.gif"><img src="./projects/faceSR.gif" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/1711.10703.pdf"><papertitle><b>FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors</b></papertitle></a><br>
		<b>Y. Tai*</b>, Y. Chen*, X. Liu, C. Shen, J. Yang.
		<br>
                <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2018 <b style="color: red">[Spotlight]</b>
		<br>
		<a href="https://drive.google.com/file/d/10i2NZfUyf2Yold4ABusz3Que-XN_gEEu/view"; style="color: #EE7F2D;">Paper (Official)</a>
		/
		<a href="https://arxiv.org/pdf/1711.10703.pdf"; style="color: #EE7F2D;">arXiv</a>
		/
		<a href="https://github.com/tyshiwo/FSRNet"; style="color: #EE7F2D;">Code</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/FSRNet.svg" alt="GitHub stars" title="" />
		/
		<a href="https://www.youtube.com/watch?v=RgHum6TB5aM"; style="color: #EE7F2D;">Demo</a>
		/
		<a href="https://drive.google.com/open?id=1U117O2ZyBDKVpPL06JA3g3uFtL6vDsp0"; style="color: #EE7F2D;">Slides</a>
		/
		<a href="https://drive.google.com/file/d/1JaSUxohFUz8m9ukAFzCbiT281nSG5lkY/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
		
	    </p>
		</td></tr>
		
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		 <img src="./projects/MemNet_iccv17.JPG" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"><papertitle><b>MemNet: A Persistent Memory Network for Image Restoration</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, X. Liu, C. Xu.
		<br>
                <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2017 <b style="color: red">[Spotlight]</b>
		<br>
		<a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://github.com/tyshiwo/MemNet"; style="color: #EE7F2D;">Code</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/MemNet.svg" alt="GitHub stars" title="" />
		/
		<a href="https://drive.google.com/open?id=1maokCHq_EzOzft8UtaAFOOUzpgE6ehuk"; style="color: #EE7F2D;">Poster</a>
	        /
	        <a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/"; style="color: #EE7F2D;"> <b>'15 most influential papers' in ICCV 2017 by PaperDigest</b></a>
		</p>
                <p><font color="red">My second paper that achieves over 1,500 google scholar citations</font></p> 
	        </p>
		</td></tr>					
		
		<td style="padding:20px;width:25%;vertical-align:middle">
		 <img src="./projects/DRRN_cvpr17.JPG" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"><papertitle><b>Image Super-Resolution via Deep Recursive Residual Network</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, X. Liu.
		<br>
                <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2017 
		<br>
		<a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"; style="color: #EE7F2D;">Paper</a>
		/
		<a href="https://github.com/tyshiwo/DRRN_CVPR17"; style="color: #EE7F2D;">Code</a>
		<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/DRRN_CVPR17.svg" alt="GitHub stars" title="" />
		/
		<a href="http://cvlab.cse.msu.edu/project-super-resolution.html"; style="color: #EE7F2D;">Project</a>
		/
		<a href="https://drive.google.com/file/d/1rWtZL3rQK5uzlw5lyPabop7MrbE0KS7j/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
		
	    </p>
               <p><font color="red">My first paper that achieves over 2,000 google scholar citations</font></p> 
	    </p>
		</td></tr>	


        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/NMR_pami16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"><papertitle><b>Nuclear Norm based Matrix Regression with Applications to Face Recognition with Occlusion and Illumination Changes</b></papertitle></a><br>
		J. Yang, L. Luo, J. Qian, <b>Y. Tai</b>, F. Zhang and Y. Xu.
		<br>
        <em>IEEE Trans. on Pattern Analysis and Machine Intelligence</em>, 2017
		<br>
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>	
		
				
        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/SOPR_SDM16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"><papertitle><b>Structural Orthogonal Procrustes Regression for Face Recognition with Pose Variations and Misalignment</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, F. Zhang, Y. Zhang, L. Luo, J. Qian.
		<br>
        <em>SIAM Conference on Data Mining (<b>SDM</b>)</em>, 2016 <b style="color: red">[Oral]</b> 
		<br>
		<a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>		
		
        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/OPR_TIP16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"><papertitle><b>Face Recognition with Pose Variations and Misalignment via Orthogonal Procrustes Regression</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, Y. Zhang, L. Luo, J. Qian and Y. Chen
		<br>
                <em>IEEE Trans. on Image Processing</em>, 2016
		<br>
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>		
		
        <td style="padding:20px;width:25%;vertical-align:middle">
		  <img src="./projects/LDSVDR_PR2016.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
		  <td style="padding:10px;width:75%;vertical-align:middle">
		<a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"><papertitle><b>Learning Discriminative Singular Value Decomposition Representation for Face Recognition</b></papertitle></a><br>
		<b>Y. Tai</b>, J. Yang, L. Luo, F. Zhang and J. Qian
		<br>
       		<em>Pattern Recognition</em>, 2016
		<br>
		<a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"; style="color: #EE7F2D;">Paper</a>
	    <br>	    
	    </p>
		</td></tr>				
						       
</table></p>
		
	<p style="text-align:left"><strong><font size="4px">Awards</font></strong></p>
		<ul>
		    <li><p style="text-align:left">2021 First Prize of Progress in Science and Technology of Jiangsu Province (4/11), <a href="http://www.jiangsu.gov.cn/module/download/downfile.jsp?classid=0&filename=d926727e803e49b788d4d7e909a5949e.pdf"; style="color: #EE7F2D;"> <b>“Image restoration and robust recognition: theory and algorithms”</b></a></p></li>
		    <li><p style="text-align:left">2021 Winner of CVPR NTIRE 2021 Challenge on Video Super-Resolution: Spatial-Temporal (Team name: Imagination)</p></li>
		    <li><p style="text-align:left">2020 Winner of CVPR NTIRE 2020 Challenge on Real-World Super-Resolution (Team name: Impressionism)</p></li>
		    <li><p style="text-align:left">2018 Stars of Youtu Lab, Tencent </p></li>
			<li><p style="text-align:left">2018, 2019, 2020, 2022 Outstanding Staff Award, Tencent </p></li>
			<li><p style="text-align:left">2018 Excellent Doctoral Dissertation of Nanjing University of Science and Technology, China </p></li>
			<li><p style="text-align:left">ICCV'17 Student Volunteer Travel Award </p></li>
			<li><p style="text-align:left">2017 Outstanding Graduate </p></li>
			<li><p style="text-align:left">2016 National Graduate Scholarship </p></li>
		</ul><br>

	<p style="text-align:justify"><strong><font size="4px">Professional activities</font></strong></p>
		<ul>
			<li><p style="text-align:left">Area Chairs for <a href="https://eccv2022.ecva.net/organizers/"; style="color: #EE7F2D;"> <b>ECCV 2022</b></a>, <a href="https://wacv2023.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2023</b></a>, <a href="https://wacv2024.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2024</b></a>, <a href="http://fg2023.ieee-biometrics.org/organizers/"; style="color: #EE7F2D;"> <b>FG 2023</b></a> </p></li>
			<li><p style="text-align:left">Reviewer for CVPR'(17, 18, 19, 20, 21, 22), ICCV'(17, 19, 21), ECCV'(18, 20), AAAI'(19, 20, 21, 22), ICLR'(20, 21), NIPS'(20, 21) </p></li>
			<li><p style="text-align:left">Reviewer for Trans. on Pattern Analysis and Machine Intelligence (TPAMI), International Journal of Computer Vision (IJCV), IEEE Trans. on Image Processing (TIP), Pattern Recognition, Pattern Recognition Letters </p></li>
	    </ur>

<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=faf9f9&w=300&t=tt&d=hvoHWGKZcyFl6zhd6aLhpusD9f4jQY_gzPG8UfsmW0I&co=1285d6'></script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha3848gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="script.js"></script><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
              Last modified in July 2023.
              For the style of my personal website, Please refer to the wonderful page from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              <br>
              </p>
            </td>
          </tr>
        </tbody></table>

</body>          

</html>
