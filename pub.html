
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="nju.css" type="text/css" />
<title>Selected Pulications </title>
</head>
<body>
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f85ae40b6d54f8f7becb3b0be41d4515";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="image-container">
    <img src="./projects/1200px-NJU.svg.png" width="90px" height="112px" alt="NJU">
</div>
<div class="menu-item"><a href="index.html">Homepage</a></div>
<div class="menu-item"><a href="pub.html" class="current">Publications</a></div>
<div class="menu-item"><a href="group.html">Members</a></div>
<div class="menu-item"><a href="service.html">Services</a></div>
<div class="menu-item"><a href="award.html">Awards</a></div>
<!-- <div class="menu-item"><a href="join.html">Join&nbsp;us</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Selected Pulications </h1>
</div>
<p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <b><span style="font-family:Wingdings">*</span></b> corresponding author. All publications on [<a href="https://scholar.google.com/citations?user=NKaiUasAAAAJ&hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a>])
<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/AddSR-arXiv2024.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2404.01717.pdf">
<papertitle><b>AddSR: Accelerating Diffusion-based Blind Super-Resolution with Adversarial Diffusion Distillation</b></papertitle>
</a>
<br>
R. Xie, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, K. Zhang, Z. Zhang, J. Zhou and J. Yang<b><span style="font-family:Wingdings">*</span></b>.              
<br>
<em>arXiv:2404.01717v1</em>, 2024
<br>
<a href="https://arxiv.org/pdf/2404.01717.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://nju-pcalab.github.io/projects/AddSR/"; style="color: #EE7F2D;">Website</a>
/
<a href="https://github.com/NJU-PCALab/AddSR"; style="color: #EE7F2D;">Code (coming soon)</a>
<p></p>
</td><tr>
  
<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/PortraitBooth-arXiv2023.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2312.06354.pdf">
<papertitle><b>PortraitBooth: A Versatile Portrait Model for Fast Identity-preserved Personalization</b></papertitle>
</a>
<br>
X. Peng, J. Zhu, B. Jiang, <b>Y. Tai</b>, D. Luo, J. Zhang, W. Lin, T. Jin, C. Wang, and R. Ji.              
<br>
<em>arXiv:2312.06354v1</em>, 2023
<br>
<a href="https://arxiv.org/pdf/2312.06354.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://portraitbooth.github.io/"; style="color: #EE7F2D;">Website</a>
<p></p>
</td><tr>
  
<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/WaveletVFI_TIP2023.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2309.03508.pdf">
<papertitle><b>Dynamic Frame Interpolation in Wavelet Domain</b></papertitle>
</a>
<br>
L. Kong, B. Jiang, D. Luo, W. Chu, <b>Y. Tai</b>, C. Wang, and J. Yang.              
<br>
<em>IEEE Trans. on Image Processing</em>, 2023
<br>
<a href="https://arxiv.org/pdf/2309.03508.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/ltkong218/WaveletVFI"; style="color: #EE7F2D;">Evaluation code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ltkong218/WaveletVFI.svg" alt="GitHub stars" title="" />
<p></p>
</td><tr>
  
<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/ImAR-arXIv2023.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2203.08612.pdf">
<papertitle><b>Learning Versatile 3D Shape Generation with Improved AR Models</b></papertitle>
</a>
<br>
S. Luo, X. Qian, Y. Fu, Y. Zhang, <b>Y. Tai</b>, Z. Zhang, C. Wang, and X. Xue.              
<br>
<em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2023
<br>
<a href="https://arxiv.org/pdf/2303.14700.pdf"; style="color: #EE7F2D;">arXiv</a>
<p></p>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/NPF-CVPR2023.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1r8jcuDpYAnwRM9HOaVJr9BgvRf9L0Vks/view?usp=share_link">
<papertitle><b>Learning Neural Proto-face Field for Disentangled 3D Face Modeling In the Wild</b></papertitle>
</a>
<br>
Z. Zhang, R. Chen, W. Cao, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, and C. Wang<b><span style="font-family:Wingdings">*</span></b>
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
<br>
<a href="https://drive.google.com/file/d/1r8jcuDpYAnwRM9HOaVJr9BgvRf9L0Vks/view?usp=share_link"; style="color: #EE7F2D;">Paper</a>
</td><tr>
      
      
<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/CALoss-CVPR2023.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1mKZ8e78DtXATv8_oZaB4d-LMBrc6UeEb/view?usp=share_link">
<papertitle><b>Learning to Measure the Point Cloud Reconstruction Loss in a Representation Space</b></papertitle>
</a>
<br>
T. Huang, Z. Ding, J. Zhang, <b>Y. Tai</b>, Z. Zhang, M. Chen, C. Wang, and Y. Liu
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
<br>
<a href="https://drive.google.com/file/d/1mKZ8e78DtXATv8_oZaB4d-LMBrc6UeEb/view?usp=share_link"; style="color: #EE7F2D;">Paper</a>
</td><tr>


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/HiTalk-CVPR2023.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2305.02572.pdf">
<papertitle><b>High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning</b></papertitle>
</a>
<br>
C. Xu, J. Zhang, J. Zhu, W. Chu, <b>Y. Tai</b>, C. Wang, and Y. Liu
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
<br>
<a href="https://arxiv.org/pdf/2305.02572.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/HitNet_AAAI2023.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2203.11624.pdf">
<papertitle><b>High-resolution Iterative Feedback Network for Camouflaged Object Detection</b></papertitle>
</a>
<br>
X. Hu, S. Wang, X. Qian, H. Dai, W. Ren, D. Luo, <b>Y. Tai</b>, and L. Shao
<br>
<em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2023
<br>
<a href="https://arxiv.org/pdf/2203.11624.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
</td><tr>


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/CRI_AAAI2023.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2302.03406.pdf">
<papertitle><b>High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets</b></papertitle>
</a>
<br>
Y. Wang, C. Lin, D. Luo, <b>Y. Tai</b>, Z. Zhang and Y. Xie
<br>
<em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2023 <b style="color: red">[Oral]</b> 
<br>
<a href="https://arxiv.org/pdf/2302.03406.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
</td><tr>   


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/3QNet_TOG2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1Sl0ttF1I4uuH7WpZ89hqFliIrSRnkcYT/view?usp=sharing">
<papertitle><b>3QNet: 3D Point Cloud Geometry Quantization Compression Network</b></papertitle>
</a>
<br>
T. Huang, J. Zhang, J. Chen, Z. Ding, <b>Y. Tai</b>, Z. Zhang, C. Wang, and Y. Liu
<br>
<em>ACM Transactions on Graphics</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1Sl0ttF1I4uuH7WpZ89hqFliIrSRnkcYT/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/ColorFormer_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/19jhYLzOsCpDsG1ntvg6rfV-4NzHPKEwW/view?usp=sharing">
<papertitle><b>ColorFormer: Image Colorization via Color Memory assisted Hybrid-attention Transformer</b></papertitle>
</a>
<br>
X. Ji*, B. Jiang*, D. Luo, G. Tao, W. Chu, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, Z. Xie, and C. Wang<b><span style="font-family:Wingdings">*</span></b>
<br>
<em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/19jhYLzOsCpDsG1ntvg6rfV-4NzHPKEwW/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1-hhTHW9aQ60JJqYKIs-6tEB1xtesjG60/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/StyleFace_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1E2VzLtjWAM9I4ZkvveSLMqXz-frf707C/view?usp=sharing">
<papertitle><b>StyleFace: Towards Identity-Disentangled Face Generation on Megapixels</b></papertitle>
</a>
<br>
Y. Luo, J. Yan, J. Zhu, K. He, W. Chu,  <b>Y. Tai</b>, and C. Wang
<br>
<em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1E2VzLtjWAM9I4ZkvveSLMqXz-frf707C/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1ncWoQXQRa7Sg_dKSpbycX0x2ieHlHZJB/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>  
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/FRS_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="google.com">
<papertitle><b>Designing One Unified Framework for High-Fidelity Face Reenactment and Swapping</b></papertitle>
</a>
<br>
C. Xu*, J.  Zhang*, Y. Han, G. Tian, X. Zeng, <b>Y. Tai</b>, Y. Wang, C. Wang, and Y. Liu
<br>
<em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1uahR40VteHhBdjRVEN_hPwN9g5r54GZ1/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1CDDtFN6txDdALMkq0Qpc2lFGstmtnntv/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
/
<a href="https://github.com/xc-csc101/UniFace"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/xc-csc101/UniFace.svg" alt="GitHub stars" title="" />
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/SeedFormer_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2207.10315.pdf">
<papertitle><b>SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer</b></papertitle>
</a>
<br>
H. Zhou, Y. Cao, W. Chu, J. Zhu, L. Tong, <b>Y. Tai</b>, and C. Wang
<br>
<em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
<br>
<a href="https://arxiv.org/pdf/2207.10315.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
/
<a href="https://drive.google.com/file/d/1p2ubWO25o9e0d0hez6hmzXahww5j_dQc/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
/
<a href="https://github.com/hrzhou2/seedformer"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/hrzhou2/seedformer.svg" alt="GitHub stars" title="" />
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/ProCA_ECCV2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2207.06654.pdf">
<papertitle><b>Prototypical Contrast Adaptation for Domain Adaptive Semantic Segmentation</b></papertitle>
</a>
<br>
Z. Jiang, Y. Li, C. Yang, P. Gao, Y. Wang, <b>Y. Tai</b>, and C. Wang
<br>
<em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
<br>
<a href="https://arxiv.org/pdf/2207.06654.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
/
<a href="https://github.com/jiangzhengkai/ProCA"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/jiangzhengkai/ProCA.svg" alt="GitHub stars" title="" />
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/CDSR_MM22.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2208.13436.pdf">
<papertitle><b>Joint Learning Content and Degradation Aware Feature for Blind Super-Resolution</b></papertitle>
</a>
<br>
Y. Zhou*, C. Lin*, D. Luo, Y. Liu, Mingang Chen, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, and C. Wang<b><span style="font-family:Wingdings">*</span></b>
<br>
<em>ACM International Conference on Multimedia (<b>ACM MM</b>)</em>, 2022
<br>
<a href="https://arxiv.org/pdf/2208.13436.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/AutoGAN-Synthesizer_MICCAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://link.springer.com/chapter/10.1007/978-3-031-16446-0_38">
<papertitle><b>AutoGAN-Synthesizer: Neural Architecture Searchfor Cross-Modality MRI Synthesis</b></papertitle>
</a>
<br>
X. Hu, R. Shen, D. Luo, <b>Y. Tai</b>, C. Wang, and B. Menze
<br>
<em>International Conference on Medical Image Computing and Computer Assisted Intervention (<b>MICCAI</b>)</em>, 2022
<br>
<a href="https://link.springer.com/chapter/10.1007/978-3-031-16446-0_38"; style="color: #EE7F2D;">Paper (official link)</a>
/
<a href="https://github.com/HUuxiaobin/AutoGAN-Synthesizer"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/HUuxiaobin/AutoGAN-Synthesizer.svg" alt="GitHub stars" title="" />
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/HifiHead_IJCAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1ZLG3hQuXJ9IkOqvhKyMHuPvS55sJXud3/view?usp=sharing">
<papertitle><b>HifiHead: One-Shot High Fidelity Neural Head Synthesis with 3D Control</b></papertitle>
</a>
<br>
F. Zhu, J. Zhu, W. Chu, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, Z. Xie, X. Huang, and C. Wang<b><span style="font-family:Wingdings">*</span></b>.        
<br>
<em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1ZLG3hQuXJ9IkOqvhKyMHuPvS55sJXud3/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/SGPN_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/18f2Spu3Qx1gskjdGh1Edx1ZlWjlC8xGB/view?usp=sharing">
<papertitle><b>Blind Face Restoration via Integrating Face Shape and Generative Priors</b></papertitle>
</a>
<br>
F. Zhu, J. Zhu, W. Chu, X. Zhang, X. Ji, C. Wang<b><span style="font-family:Wingdings">*</span></b>, and <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>.        
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/18f2Spu3Qx1gskjdGh1Edx1ZlWjlC8xGB/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1BhJa5I95sAzENzo0_He-L8vcsYFlKL9N/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
/
<a href="https://github.com/TencentYoutuResearch/FaceRestoration-sgpn"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/FaceRestoration-sgpn.svg" alt="GitHub stars" title="" />
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/IFRNet_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1oj4uCqu7E2913JKLohu7nZsj5SvECCdq/view?usp=sharing">
<papertitle><b>IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation</b></papertitle>
</a>
<br>
L. Kong*, B. Jiang*, D. Luo, W. Chu, X. Huang, <b>Y. Tai</b>, C. Wang, and J. Yang.        
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1oj4uCqu7E2913JKLohu7nZsj5SvECCdq/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/15K_m9HhGQ4MfrUREr0DxcdtEuCnv_OD3/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>    
/
<a href="https://github.com/ltkong218/IFRNet"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ltkong218/IFRNet.svg" alt="GitHub stars" title="" />
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/phyDIR_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1s-6glim2XEKNGmrRJKrXyKFJ1X58B7GA/view?usp=sharing">
<papertitle><b>Physically-guided Disentangled Implicit Rendering for 3D Face Modeling</b></papertitle>
</a>
<br>
Z. Zhang, Y. Ge, <b>Y. Tai</b>, W. Cao, R. Chen, K. Liu, H. Tang, X. Huang, C. Wang, Z. Xie, and D. Huang.        
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1s-6glim2XEKNGmrRJKrXyKFJ1X58B7GA/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1UohmPBmgoo6-Dh7ES2ugLvy9-X2TVDNi/view?usp=sharing"; style="color: #EE7F2D;">Supp</a> 
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/RDF_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/14A4zxu6u5vQ9kROA4HvWllRQ72P8Ml6A/view?usp=sharing">
<papertitle><b>Learning to Restore 3D Face from In-the-Wild Degraded Images</b></papertitle>
</a>
<br>
Z. Zhang, Y. Ge, <b>Y. Tai</b>, X. Huang, C. Wang, H. Tang, D. Huang, and Z. Xie.        
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/14A4zxu6u5vQ9kROA4HvWllRQ72P8Ml6A/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1ahwfZtPB4YrLWXswsDz2jSPGon2ZCiZa/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/MFH_CVPR2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1Gq-167f2ue30463K4XMkHOJThg9Yugk0/view?usp=sharing">
<papertitle><b>Learning to Memorize Feature Hallucination for One-Shot Image Generation</b></papertitle>
</a>
<br>
Y. Xie, Y. Fu, J. Zhu, <b>Y. Tai</b>, Y. Cao, and C. Wang.        
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1Gq-167f2ue30463K4XMkHOJThg9Yugk0/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/DIRL-AAAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1dlX3bHVJ8-djyDeuK3B-EhJoNEwxQMSY/view?usp=sharing">
<papertitle><b>DIRL: Domain-invariant Representation Learning for Generalizable Semantic Segmentation</b></papertitle>
</a>
<br>
Q. Xu, L. Yao, Z. Jiang, G. Jiang, W. Chu, W. Han, W. Zhang, C. Wang, and <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>.        
<br>
<em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022 <b style="color: red">[Oral]</b> 
<br>
<a href="https://drive.google.com/file/d/1dlX3bHVJ8-djyDeuK3B-EhJoNEwxQMSY/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/SCSNet-AAAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1LOo18uoqCBZ_fb3HJYOcBVmUMBEVCZRj/view?usp=sharing">
<papertitle><b>SCSNet: Simultaneously Image Colorization and Super-Resolution</b></papertitle>
</a>
<br>
J. Zhang, C. Xu, Y. Han, J. Li, Y. Wang, <b>Y. Tai</b>, C. Wang, F. Huang, Z. Xie, and Y. Liu.      
<br>
<em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1LOo18uoqCBZ_fb3HJYOcBVmUMBEVCZRj/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/LCTR-AAAI2022.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1aCz2WOuxa3ppYkb2i43i6_WamA9drLU9/view?usp=sharing">
<papertitle><b>LCTR: On Awakening the Local Continuity of Transformer for Weakly Supervised Object Localization</b></papertitle>
</a>
<br>
Z. Chen, C. Wang, Y. Wang, G. Jiang, Y. Shen, <b>Y. Tai</b>, C. Wang, W. Zhang, and L. Cao.       
<br>
<em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1aCz2WOuxa3ppYkb2i43i6_WamA9drLU9/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/S2K-nips2021.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2110.12151.pdf">
<papertitle><b>Spectrum-to-Kernel Translation for Accurate Blind Image Super-Resolution</b></papertitle>
</a>
<br>
G. Tao, X. Ji, W. Wang, S. Chen, C. Lin, Y. Cao, T. Lu, D. Luo, and <b>Y. Tai</b>.             
<br>
<em>Thirty-fifth Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/pdf/2110.12151.pdf"; style="color: #EE7F2D;">arXiv</a>
<p><font color="red">A novel framework S2K that predicts the kernel from spectrum in frequency domain</p>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/EAT-arXiv2021.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2105.15089.pdf">
<papertitle><b>Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model</b></papertitle>
</a>
<br>
J. Zhang, C. Xu, J. Li, W. Chen, Y. Wang, <b>Y. Tai</b>, S. Chen, C. Wang, F. Huang and R. Liu.             
<br>
<em>Thirty-fifth Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/pdf/2105.15089.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/TencentYoutuResearch/BaseArchitecture-EAT"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/BaseArchitecture-EAT.svg" alt="GitHub stars" title="" />
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/P2PNet-ICCV2021.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2107.12746.pdf">
<papertitle><b>Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework</b></papertitle>
</a>
<br>
Q. Song*, C. Wang*, Z. Jiang, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Wu.              
<br>
<em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2021 <b style="color: red">[Oral]</b> 
<br>
<a href="https://arxiv.org/pdf/2107.12746.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-P2PNet.svg" alt="GitHub stars" title="" />
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/UEPNet-ICCV2021.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2107.12619.pdf">
<papertitle><b>Uniformity in Heterogeneity: Diving Deep into Count Interval Partition for Crowd Counting</b></papertitle>
</a>
<br>
C. Wang*, Q. Song*, B. Zhang, Y. Wang, <b>Y. Tai</b>, X. Hu, C. Wang, J. Li, J. Ma, and Y. Wu.             
<br>
<em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/pdf/2107.12619.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-UEPNet.svg" alt="GitHub stars" title="" />
</td><tr>


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/ASFD_arXiv20.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2003.11228.pdf">
<papertitle><b>ASFD: Automatic and Scalable Face Detector</b></papertitle>
</a>
<br>
J. Li*, B. Zhang*, Y. Wang, <b>Y. Tai</b>, Z. Zhang, C. Wang, J. Li, X. Huang and Y. Xia.              
<br>
<em>ACM International Conference on Multimedia (<b>ACM MM</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;">arXiv</a>
<p><font color="red">Ranked No. 1 on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a></font></p>
</td><tr>


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/HifiFace-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2106.09965.pdf">
<papertitle><b>HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping</b></papertitle>
</a>
<br>
Y. Wang*, X. Chen*, J. Zhu, W. Chu, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, C. Wang, J. Li, Y. Wu, F. Huang and R. Ji.              
<br>
<em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
<br>
<a href="https://arxiv.org/pdf/2106.09965.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://johann.wang/HifiFace/"; style="color: #EE7F2D;">Project</a>
/
<a href="https://drive.google.com/file/d/1K9QsPX2Yw7iUtH33aoP-dTlQhnZLQuWC/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>  
/
<a href="https://drive.google.com/file/d/19i-4tJD7NxrqtmlRUl9l3M-FuMy7F2EG/view?usp=sharing"; style="color: #EE7F2D;">Video (1min)</a>  
</p>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/SPL-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2106.07220.pdf">
<papertitle><b>Context-Aware Image Inpainting with Learned Semantic Priors</b></papertitle>
</a>
<br>
W. Zhang, J. Zhu, <b>Y. Tai</b>, Y. Wang, W. Chu, B. Ni, C. Wang and X. Yang.              
<br>
<em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
<br>
<a href="https://arxiv.org/pdf/2106.07220.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://arxiv.org/pdf/2112.04107.pdf"; style="color: #EE7F2D;">Extended journal version</a>
/
<a href="https://github.com/WendongZh/SPL"; style="color: #EE7F2D;">Code (Official)</a>    
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/WendongZh/SPL.svg" alt="GitHub stars" title="" />
</p>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/DRDG-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2106.16128.pdf">
<papertitle><b>Dual Reweighting Domain Generalization for Face Presentation Attack Detection</b></papertitle>
</a>
<br>
S. Liu, K. Zhang, T. Yao, K. Sheng, S. Ding, <b>Y. Tai</b>, J. Li, Y. Xie and L. Ma.              
<br>
<em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
<br>
<a href="https://arxiv.org/pdf/2106.16128.pdf"; style="color: #EE7F2D;">arXiv</a>
</p>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/SiamRCR-IJCAI2021.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2105.11237.pdf">
<papertitle><b>SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking</b></papertitle>
</a>
<br>
J. Peng*, Z. Jiang*, Y. Gu*, Y. Wu, Y. Wang, <b>Y. Tai</b>, C. Wang and W. Lin.              
<br>
<em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
<br>
<a href="https://arxiv.org/pdf/2105.11237.pdf"; style="color: #EE7F2D;">arXiv</a>
</p>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/3DFace_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Aggregate_and_Personalize_3D_Face_From_In-the-Wild_Photo_CVPR_2021_paper.pdf">
<papertitle><b>Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection</b></papertitle>
</a>
<br>
Z. Zhang, Y. Ge, R. Chen, <b>Y. Tai</b>, Y. Yan, J. Yang, C. Wang, J. Li, and F. Huang.              
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 <b style="color: red">[Oral]</b> 
<br>
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Aggregate_and_Personalize_3D_Face_From_In-the-Wild_Photo_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/TencentYoutuResearch/3DFaceReconstruction-LAP"; style="color: #EE7F2D;">Code (Official)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/3DFaceReconstruction-LAP.svg" alt="GitHub stars" title="" />
</p>
</td><tr>   


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/AFSD_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/abs/2103.13137">
<papertitle><b>Learning Salient Boundary Feature for Anchor-free Temporal Action Localization</b></papertitle>
</a>
<br>
C. Lin*, C. Xu*, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.              
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/abs/2103.13137"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Learning_Salient_Boundary_Feature_for_Anchor-free_Temporal_Action_Localization_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/TencentYoutuResearch/ActionDetection-AFSD"; style="color: #EE7F2D;">Code (Official)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/ActionDetection-AFSD.svg" alt="GitHub stars" title="" />
</p>
</td><tr> 

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/REVIDE_CVPR2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Restore_Hazy_Video_A_New_Real-World_Dataset_and_CVPR_2021_paper.pdf"><papertitle><b>Learning to Restore Hazy Video: A New Real-World Dataset and A New Method</b></papertitle></a><br>
X. Zhang*, H. Dong*, J. Pan, C. Zhu, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and F. Wang.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 
<br>
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Restore_Hazy_Video_A_New_Real-World_Dataset_and_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>

</td></tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/FCA_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2012.10102.pdf"><papertitle><b>Frequency Consistent Adaptation for Real World Super Resolution</b></papertitle></a><br>
X. Ji*, G. Tao*, Y. Cao, <b>Y. Tai</b>, T. Lu, C. Wang, J. Li, and F. Huang.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/pdf/2012.10102.pdf"; style="color: #EE7F2D;">arXiv</a>
</p>
<p><font color="red">Improved version of our prior work RealSR</font></p>
</td></tr>        

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/CMR_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"><papertitle><b>Learning Comprehensive Motion Representation for Action Recognition</b></papertitle></a><br>
M. Wu*, B. Jiang*, D. Luo, J. Yan, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, and X. Yang.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
<br>
<a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/TencentYoutuResearch/ActionRecognition-CMR"; style="color: #EE7F2D;">Code (Official)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/ActionRecognition-CMR.svg" alt="GitHub stars" title="" />
</p>
<p><font color="red">Extented version of our prior works TEINet and TDRL</font></p>
</td></tr>  


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/D2AM_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2105.02453.pdf"><papertitle><b>Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing</b></papertitle></a><br>
Z. Chen, T. Yao, K. Sheng, S. Ding, <b>Y. Tai</b>, J. Li, F. Huang, and X. Jin.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
<br>
<a href="https://arxiv.org/pdf/2105.02453.pdf"; style="color: #EE7F2D;">arXiv</a>
</p>
</td></tr>  

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/SASNet_AAAI2021.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"><papertitle><b>To Choose or to Fuse? Scale Selection for Crowd Counting</b></papertitle></a><br>
Q. Song*, C. Wang*, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, J. Wu, and J. Ma.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
<br>
<a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/TencentYoutuResearch/CrowdCounting-SASNet"; style="color: #EE7F2D;">Code (Official)</a> 
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-SASNet" alt="GitHub stars" title="" />
</p>
</td></tr>  

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/FAN_ArXiv19.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/1911.11680.pdf"><papertitle><b>FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization</b></papertitle></a><br>
X. Yin, <b>Y. Tai</b>, Y. Huang and X. Liu.
<br>
<em>Asian Conference on Computer Vision (<b>ACCV</b>)</em>, 2020
<br>
<a href="https://arxiv.org/pdf/1911.11680.pdf"; style="color: #EE7F2D;">Paper</a>
</p>
<p><font color="red">Novel framework to improve surveillance face recognition & normalization from unpaired data</font></p>
</td></tr>  

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/DDL_Arxiv2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2002.03662.pdf"><papertitle><b>Improving Face Recognition from Hard Samples via Distribution Distillation Loss</b></papertitle></a><br>
Y. Huang*, P. Shen*, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, S. Li<b><span style="font-family:Wingdings">*</span></b>, X. Liu, J. Li, F. Huang, and R. Ji.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
<br>
<a href="https://arxiv.org/pdf/2002.03662.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750137.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/Tencent/TFace"; style="color: #EE7F2D;">Code (Official)</a> 
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/TFace.svg" alt="GitHub stars" title="" />
</p>
</td></tr>        

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/SSCGAN_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"><papertitle><b>SSCGAN: Facial Attribute Editing via Style Skip Connections</b></papertitle></a><br>
W. Chu, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, C. Wang, J. Li, F. Huang, and R. Ji.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
<br>
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
</p>
</td></tr>               


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/DRL_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2008.08250.pdf"><papertitle><b>Face Anti-Spoofing via Disentangled Representation Learning</b></papertitle></a><br>
K. Zhang, T. Yao, J. Zhang, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, S. Ding, J. Li, F. Huang, H. Song and L. Ma.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
<br>
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640630.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
</p>
</td></tr>  

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/CTracker_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2007.14557.pdf"><papertitle><b>Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End <br> Joint Multiple-Object Detection and Tracking</b></papertitle></a><br>
J. Peng, C. Wang, F. Wan, Y. Wu, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020 <b style="color: red">[Spotlight]</b> 
<br>
<a href="https://arxiv.org/pdf/2007.14557.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490137.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/pjl1995/CTracker"; style="color: #EE7F2D;">Code (Official)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/pjl1995/CTracker.svg" alt="GitHub stars" title="" />

</p>
</td></tr>      

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/TDRL_ECCV2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2007.07626.pdf"><papertitle><b>Temporal Distinct Representation Learning for 2D-CNN-based Action Recognition</b></papertitle></a><br>
J. Weng, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, X. Jiang and J. Yuan.        <br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
<br>
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520358.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
</p>
</td></tr>  

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/HPE_arXiv2019.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2008.00697.pdf"><papertitle><b>Adversarial Semantic Data Augmentation for Human Pose Estimation</b></papertitle></a><br>
Y. Bin, X. Cao, X. Chen, Y. Ge, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, C. Gao and N. Sang.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
<br>
<a href="https://arxiv.org/pdf/2008.00697.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640596.pdf"; style="color: #EE7F2D;">Paper (Official)</a>  
/
<a href="https://github.com/Binyr/ASDA"; style="color: #EE7F2D;">Code (Official)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Binyr/ASDA.svg" alt="GitHub stars" title="" />

</p>
<p><font color="red">State-of-the-art performance on <a href="http://human-pose.mpi-inf.mpg.de/#results"; style="color: #EE7F2D;"> <b>MPII</b></a></font> and <a href="http://sam.johnson.io/research/lsp.html"; style="color: #EE7F2D;"> <b>LSP</b></a></font></p>
</td></tr>        


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/RealSR_CVPRW2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2005.01996.pdf"><papertitle><b>Real-World Super-Resolution via Kernel Estimation and Noise Injection</b></papertitle></a><br>
X. Ji, Y. Cao, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, C. Wang, J. Li, and F. Huang.
<br>
<em>Computer Vision and Pattern Recognition Workshop (<b>CVPRW</b>)</em>, 2020
<br>
<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/Tencent/Real-SR"; style="color: #EE7F2D;">Code (Tencent)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/Real-SR.svg" alt="GitHub stars" title="" />
/
<a href="https://github.com/Tencent/Real-SR"; style="color: #EE7F2D;">Code (Personal)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/jixiaozhong/RealSR.svg" alt="GitHub stars" title="" />
/
<a href="https://github.com/nihui/realsr-ncnn-vulkan"; style="color: #EE7F2D;">Code (NCNN-vulkan)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/nihui/realsr-ncnn-vulkan.svg" alt="GitHub stars" title="" />
</p>
<p><font color="red">Winner of <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a></font></p>
</td></tr>        

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/CurricularFace_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"><papertitle><b>CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition</b></papertitle></a><br>
Y. Huang, Y. Wang, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, X. Liu, P. Shen, S. Li<b><span style="font-family:Wingdings">*</span></b>, J. Li, and F. Huang.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
<br>
<a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/HuangYG123/CurricularFace"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/HuangYG123/CurricularFace.svg" alt="GitHub stars" title="" />

</p>
</td></tr>      

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/UnsupervisedOpticalFlow_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"><papertitle><b>Learning by Analogy: Reliable Supervision from Transformations for <br> Unsupervised Optical Flow Estimation</b></papertitle></a><br>
L. Liu, J. Zhang, Y. Liu, Y. Wang, <b>Y. Tai</b>, D. Luo, C. Wang, J. Li, and F. Huang.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
<br>
<a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/lliuz/ARFlow"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/lliuz/ARFlow.svg" alt="GitHub stars" title="" />

</p>
</td></tr>  

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/VideoReID_CVPR2020.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"><papertitle><b>Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification</b></papertitle></a><br>
Y. Yan, J. Qin, J. Chen, L. Liu, F. Zhu, <b>Y. Tai</b>, and L. Shao.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
<br>
<a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/daodaofr/hypergraph_reid"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/daodaofr/hypergraph_reid.svg" alt="GitHub stars" title="" />

</p>
</td></tr>  


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/DBG_AAAI20.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/1911.04127.pdf"><papertitle><b>Fast Learning of Temporal Action Proposal via Dense Boundary Generator</b></papertitle></a><br>
C. Lin*, J. Li*, Y. Wang, <b>Y. Tai</b>, D. Luo, Z. Cui, C. Wang, J. Li, F. Huang and R. Ji.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2020
<br>
<a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/Tencent/ActionDetection-DBG"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/ActionDetection-DBG.svg" alt="GitHub stars" title="" />

<p><font color="red">Ranked No. 1 on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a></font></p> 
</p>
</td></tr>          

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/TEI_AAAI20.png" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/1911.09435.pdf"><papertitle><b>TEINet: Towards an Efficient Architecture for Video Recognition</b></papertitle></a><br>
Z. Liu*, D. Luo*, Y. Wang, L. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and T. Lu.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2020
<br>
<a href="https://arxiv.org/pdf/1911.09435.pdf"; style="color: #EE7F2D;">arXiv</a>
<br>      
</p>
</td></tr>  


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/face_detection_arxiv18.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://128.84.21.199/abs/1810.10220"><papertitle><b>DSFD: Dual Shot Face Detector</b></papertitle></a><br>
J. Li, Y. Wang, C. Wang, <b>Y. Tai</b>, J. Qian, J. Yang, C.e Wang, J. Li and F. Huang.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2019
<br>
<a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://drive.google.com/open?id=1DuhcD4Thwv8E3kcLZIOQTeo8qaGDq9PB"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/Tencent/FaceDetection-DSFD"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/FaceDetection-DSFD.svg" alt="GitHub stars" title="" />

<p><font color="red">Ranked No. 1 on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a> and <a href="http://vis-www.cs.umass.edu/fddb/results.html#eval"; style="color: #EE7F2D;"> <b>FDDB</b></a> (Until 2019.01)</font></p> 
</p>
</td></tr>


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/aaai19_FHR.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/1811.00342.pdf"><papertitle><b>Towards Highly Accurate and Stable Face Alignment for High-Resolution Videos</b></papertitle></a><br>
<b>Y. Tai*</b>, Y. Liang*, X. Liu, L. Duan, J. Li, C. Wang, F. Huang and Y. Chen.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2019
<br>
<a href="https://arxiv.org/pdf/1811.00342.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://drive.google.com/open?id=1bDxP_i2ETfoO4uAbPhukCcxvOrVQuwd-"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://drive.google.com/open?id=1cFyjZWdGOBZ8t-63bZehERMaKpTkawwe"; style="color: #EE7F2D;">Supp</a>
/
<a href="https://github.com/tyshiwo/FHR_alignment"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/FHR_alignment.svg" alt="GitHub stars" title="" />

</p>
</td></tr>    


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/DAML_aaai19.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"><papertitle><b>Data-Adaptive Metric Learning with Scale Alignment</b></papertitle></a><br>
S. Chen, C. Gong, J. Yang, <b>Y. Tai</b>, L. Hui and J. Li.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2019
<br>
<a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"; style="color: #EE7F2D;">Paper (Official)</a>
<br>
</p>
</td></tr>            


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/CD_ECCV18.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"><papertitle><b>Person Search via A Mask-Guided Two-Stream CNN Model</b></papertitle></a><br>
D. Chen, S. Zhang, W. Ouyang, J. Yang and <b>Y. Tai</b>.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2018
<br>
<a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1Wg4ieK4BQc5XMZCBPpQJXumoHs21AHBM/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
<br>
</p>
</td></tr>  


<td style="padding:20px;width:25%;vertical-align:middle">
<a href="./projects/faceSR.gif"><img src="./projects/faceSR.gif" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/1711.10703.pdf"><papertitle><b>FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors</b></papertitle></a><br>
<b>Y. Tai*</b>, Y. Chen*, X. Liu, C. Shen, J. Yang.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2018 <b style="color: red">[Spotlight]</b>
<br>
<a href="https://drive.google.com/file/d/10i2NZfUyf2Yold4ABusz3Que-XN_gEEu/view"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://arxiv.org/pdf/1711.10703.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/tyshiwo/FSRNet"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/FSRNet.svg" alt="GitHub stars" title="" />
/
<a href="https://www.youtube.com/watch?v=RgHum6TB5aM"; style="color: #EE7F2D;">Demo</a>
/
<a href="https://drive.google.com/open?id=1U117O2ZyBDKVpPL06JA3g3uFtL6vDsp0"; style="color: #EE7F2D;">Slides</a>
/
<a href="https://drive.google.com/file/d/1JaSUxohFUz8m9ukAFzCbiT281nSG5lkY/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>

</p>
</td></tr>


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/MemNet_iccv17.JPG" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"><papertitle><b>MemNet: A Persistent Memory Network for Image Restoration</b></papertitle></a><br>
<b>Y. Tai</b>, J. Yang, X. Liu, C. Xu.
<br>
<em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2017 <b style="color: red">[Spotlight]</b>
<br>
<a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/tyshiwo/MemNet"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/MemNet.svg" alt="GitHub stars" title="" />
/
<a href="https://drive.google.com/open?id=1maokCHq_EzOzft8UtaAFOOUzpgE6ehuk"; style="color: #EE7F2D;">Poster</a>
/
<a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/"; style="color: #EE7F2D;"> <b>'15 most influential papers' in ICCV 2017 by PaperDigest</b></a>
</p>
<p><font color="red">My second paper that achieves over 1,500 google scholar citations</font></p> 
</p>
</td></tr>          

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/DRRN_cvpr17.JPG" style="height: 120px; width: 200px; margin-top: 10px"></a></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"><papertitle><b>Image Super-Resolution via Deep Recursive Residual Network</b></papertitle></a><br>
<b>Y. Tai</b>, J. Yang, X. Liu.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2017 
<br>
<a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/tyshiwo/DRRN_CVPR17"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/DRRN_CVPR17.svg" alt="GitHub stars" title="" />
/
<a href="http://cvlab.cse.msu.edu/project-super-resolution.html"; style="color: #EE7F2D;">Project</a>
/
<a href="https://drive.google.com/file/d/1rWtZL3rQK5uzlw5lyPabop7MrbE0KS7j/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>

</p>
<p><font color="red">My first paper that achieves over 2,000 google scholar citations</font></p> 
</p>
</td></tr>  


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/NMR_pami16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"><papertitle><b>Nuclear Norm based Matrix Regression with Applications to Face Recognition with Occlusion and Illumination Changes</b></papertitle></a><br>
J. Yang, L. Luo, J. Qian, <b>Y. Tai</b>, F. Zhang and Y. Xu.
<br>
<em>IEEE Trans. on Pattern Analysis and Machine Intelligence</em>, 2017
<br>
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"; style="color: #EE7F2D;">Paper</a>
<br>      
</p>
</td></tr>  


<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/SOPR_SDM16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"><papertitle><b>Structural Orthogonal Procrustes Regression for Face Recognition with Pose Variations and Misalignment</b></papertitle></a><br>
<b>Y. Tai</b>, J. Yang, F. Zhang, Y. Zhang, L. Luo, J. Qian.
<br>
<em>SIAM Conference on Data Mining (<b>SDM</b>)</em>, 2016 <b style="color: red">[Oral]</b> 
<br>
<a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"; style="color: #EE7F2D;">Paper</a>
<br>      
</p>
</td></tr>    

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/OPR_TIP16.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"><papertitle><b>Face Recognition with Pose Variations and Misalignment via Orthogonal Procrustes Regression</b></papertitle></a><br>
<b>Y. Tai</b>, J. Yang, Y. Zhang, L. Luo, J. Qian and Y. Chen
<br>
<em>IEEE Trans. on Image Processing</em>, 2016
<br>
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"; style="color: #EE7F2D;">Paper</a>
<br>      
</p>
</td></tr>    

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/LDSVDR_PR2016.JPG" style="height: 120px; width: 200px; margin-top: 10px"></td>
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"><papertitle><b>Learning Discriminative Singular Value Decomposition Representation for Face Recognition</b></papertitle></a><br>
<b>Y. Tai</b>, J. Yang, L. Luo, F. Zhang and J. Qian
<br>
<em>Pattern Recognition</em>, 2016
<br>
<a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"; style="color: #EE7F2D;">Paper</a>
<br>      
</p>
</td></tr>    

</table>

</td>
</tr>
</table>
</body>
</html>
