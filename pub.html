<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="nju.css" type="text/css" />
<title>Selected Pulications </title>
</head>
<body>
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f85ae40b6d54f8f7becb3b0be41d4515";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="image-container">
    <img src="./projects/1200px-NJU.svg.png" width="90px" height="112px" alt="NJU">
</div>
<div class="menu-item"><a href="index.html">Homepage</a></div>
<div class="menu-item"><a href="pub.html" class="current">Publications</a></div>
<div class="menu-item"><a href="group.html">Members</a></div>
<div class="menu-item"><a href="service.html">Services</a></div>
<div class="menu-item"><a href="award.html">Awards</a></div>
<!-- <div class="menu-item"><a href="join.html">Join&nbsp;us</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Selected Publications </h1>
</div>
<p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <b><span style="font-family:Wingdings">*</span></b> corresponding author. All publications on [<a href="https://scholar.google.com/citations?user=NKaiUasAAAAJ&hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a>]; <b>22*CVPR, 9*ICCV, 13*ECCV, 18*AAAI, 5*IJCAI, 5*NeurIPS, 1*ICLR</b>)

<ul>
<a href="google.com">
<li><p><b>UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset</b>
</a>
<br>
Chen Zhao, En Ci, Yunzhe Xu, Tiehan Fan, Shanyan Guan, Yanhao Ge, Jian Yang, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>           
<br>
<em>Neural Information Processing Systems (NeurIPS)</em>, 2025
<br>
<a href="google.com"; style="color: #EE7F2D;">Paper (coming soon)</a>
</p>
</li>
</ul>
  
<ul>
<a href="google.com">
<li><p><b>Learning Multi-scale Spatial-frequency Features for Image Denoising</b>
</a>
<br>
Xu Zhao, Chen Zhao, Xiantao Hu, Hongliang Zhang, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, Jian Yang             
<br>
<em>Pattern Recognition</em>, 2025
<br>
<a href="https://arxiv.org/pdf/2506.16307"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>
  
<ul>
<a href="google.com">
<li><p><b>AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping</b>
</a>
<br>
Ze-dong Zhang, <b>Y. Tai</b>, Jianjun Qian, Jian Yang, Jun Li.              
<br>
<em>Siggraph Asia</em>, 2025
<br>
<a href="google.com"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>
  
<ul>
<a href="google.com">
<li><p><b>AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping</b>
</a>
<br>
Ze-dong Zhang, <b>Y. Tai</b>, Jianjun Qian, Jian Yang, Jun Li.              
<br>
<em>Siggraph Asia</em>, 2025
<br>
<a href="google.com"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>

<ul>
<a href="google.com">
<li><p><b>Category-Aware 3D Object Composition with Disentangled Texture and Shape Multi-view Diffusion</b>
</a>
<br>
Zeren Xiong, Zikun Chen, Zedong Zhang, Xiang Li, <b>Y. Tai</b>, Jian Yang, Jun Li             
<br>
<em>ACM Multimedia</em>, 2025
<br>
<a href="https://arxiv.org/pdf/2509.02357?"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>
  
<ul>
<a href="google.com">
<li><p><b>Describe, Don’t Dictate: Semantic Image Editing with Natural Language Intent</b>
</a>
<br>
En Ci, Shanyan Guan, Yanhao Ge, Yilin Zhang, Wei Li, Zhenyu Zhang, Jian Yang, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>.              
<br>
<em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2025
<br>
<a href="google.com"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>

<ul>
<a href="google.com">
<li><p><b>Reverse Convolution and Its Applications to Image Restoration</b>
</a>
<br>
Xuhong Huang, Shiqi Liu, Kai Zhang, <b>Ying Tai</b>, Jian Yang, Hui Zeng, Lei Zhang            
<br>
<em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2025
<br>
<a href="google.com"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2411.06558">
<li><p><b>Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement</b>
</a>
<br>
Zhennan Chen*, Yajie Li*, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>.              
<br>
<em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2025
<br>
<a href="https://arxiv.org/pdf/2411.06558"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2501.02976?">
<li><p><b>STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution</b>
</a>
<br>
Rui Xie*, Yinhong Liu*, Penghao Zhou, Chen Zhao, Jun Zhou, Kai Zhang, Zhenyu Zhang, Jian Yang, Zhenheng Yang, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>.              
<br>
<em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2025
<br>
<a href="https://arxiv.org/pdf/2501.02976?"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2412.11586">
<li><p><b>StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors</b>
</a>
<br>
Xiaokun Sun, Zeyu Cai, <b>Ying Tai</b>, Jian Yang, Zhenyu Zhang.              
<br>
<em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2025
<br>
<a href="https://arxiv.org/pdf/2412.11586"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>

<ul>
<a href="google.com">
<li><p><b>Category-Aware 3D Object Composition with Disentangled Texture and Shape Multi-view Diffusion</b>
</a>
<br>
Zeren Xiong, Zikun Chen, Ze-dong Zhang, Xiang Li, <b>Ying Tai</b>, Jian Yang, Jun Li          
<br>
<em>ACM Multimedia</em>, 2025
<br>
<a href="google.com"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>
  
<ul>
<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_From_Zero_to_Detail_Deconstructing_Ultra-High-Definition_Image_Restoration_from_Progressive_CVPR_2025_paper.pdf">
<li><p><b>From Zero to Detail: Deconstructing Ultra-High-Definition Image Restoration from Progressive Spectral Perspective</b>
</a>
<br>
Chen Zhao, Zhizhou Chen, Yunzhe Xu, Enxuan Gu, Jian Li, Zili Yi, Qian Wang, Jian Yang, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>.              
<br>
<em>Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2025
<br>
<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_From_Zero_to_Detail_Deconstructing_Ultra-High-Definition_Image_Restoration_from_Progressive_CVPR_2025_paper.pdf"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>

<ul>
<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_InstanceCap_Improving_Text-to-Video_Generation_via_Instance-aware_Structured_Caption_CVPR_2025_paper.pdf">
<li><p><b>InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption</b>
</a>
<br>
Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Zhenheng Yang, Chaoyou Fu, Xiang Li, Jian Yang, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>.              
<br>
<em>Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2025
<br>
<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_InstanceCap_Improving_Text-to-Video_Generation_via_Instance-aware_Structured_Caption_CVPR_2025_paper.pdf"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>


<ul>
<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion_CVPR_2025_paper.pdf">
<li><p><b>Towards Universal Dataset Distillation via Task-Driven Diffusion</b>
</a>
<br>
Ding Qi, Jian Li, Junyao Gao, Shuguang Dou, <b>Ying Tai</b>, Jianlong Hu, Bo Zhao, Yabiao Wang, Chengjie Wang, Cairong Zhao.              
<br>
<em>Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2025
<br>
<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion_CVPR_2025_paper.pdf"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2407.02371">
<li><p><b>OpenVid-1M: A Large-Scale Dataset for High-Quality Text-to-Video Generation</b>
</a>
<br>
K. Nan*, R. Xie*, P. Zhou*, T. Fan, Z. Yang, Z. Chen, X. Li, J. Yang and <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>.              
<br>
<em>International Conference on Learning Representations</em>, 2025
<br>
<a href="https://arxiv.org/pdf/2407.02371"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://nju-pcalab.github.io/projects/openvid/"; style="color: #EE7F2D;">Website</a>
/
<a href="https://www.youtube.com/watch?v=ie8JlFptZ9o&t=24s"; style="color: #EE7F2D;">Demo (High-res)</a>
/
<a href="https://huggingface.co/datasets/nkp37/OpenVid-1M"; style="color: #EE7F2D;">Dataset</a>
/
<a href="https://huggingface.co/datasets/nkp37/OpenVid-1M/tree/main/model_weights"; style="color: #EE7F2D;">Models</a>
/
<a href="https://github.com/NJU-PCALab/OpenVid-1M"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/NJU-PCALab/OpenVid-1M.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2404.01717.pdf">
<li><p><b>AddSR: Accelerating Diffusion-based Blind Super-Resolution with Adversarial Diffusion Distillation</b> 
</a>
<br>
R. Xie, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, C. Zhao, K. Zhang, Z. Zhang, J. Zhou, X. Ye, Q. Wang<b><span style="font-family:Wingdings">*</span></b> and J. Yang.              
<br>
<em>arXiv:2404.01717</em>, 2024
<br>
<a href="https://arxiv.org/pdf/2404.01717"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://nju-pcalab.github.io/projects/AddSR/"; style="color: #EE7F2D;">Website</a>
/
<a href="https://github.com/NJU-PCALab/AddSR"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/NJU-PCALab/AddSR.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>


<ul>
<a href="google.com">
<li><p><b>Exploiting Multimodal Spatial-temporal Patterns for Video Object Tracking</b> 
</a>
<br>
Xiantao Hu, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, Xu zhao, Chen Zhao, Zhenyu Zhang, Jun Li, Bineng Zhong, Jian Yang<b><span style="font-family:Wingdings">*</span></b>           
<br>
<em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2025 <b style="color: red">[Oral]</b> 
<br>
<a href="https://arxiv.org/pdf/2412.15691"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/NJU-PCALab/STTrack"; style="color: #EE7F2D;">Code</a>
</p>
</li>
</ul>


<ul>
<a href="google.com">
<li><p><b>Anywhere: A Multi-Agent Framework for User-Guided, Reliable, and Diverse Foreground-Conditioned Image Generation</b> 
</a>
<br>
Xie Tianyidan, Rui Ma, qian Wang, Xiaoqian Ye, Feixuan Liu, <b>Y. Tai</b>, Zhenyu Zhang, Lanjun Wang, Zili Yi
<br>
<em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2025
<br>
</p>
</li>
</ul>

<ul>
<a href="google.com">
<li><p><b>Guided Real Image Dehazing Using YCbCr Color Space</b> 
</a>
<br>
Wenxuan Fang, Junkai Fan, Yu Zheng, Jiangwei Weng, <b>Y. Tai</b>, Jun Li  
<br>
<em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2025
<br>
</p>
</li>
</ul>


<ul>
<a href="google.com">
<li><p><b>Learning Generalized Residual Exchange-Correlation-Uncertain Functional for Density Functional Theory</b> 
</a>
<br>
Sizhuo Jin, Shuo Chen, Jianjun Qian, <b>Y. Tai</b>, Jun Li
<br>
<em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2025
<br>
</p>
</li>
</ul>


<ul>
<a href="google.com">
<li><p><b>From Words to Worth: Newborn Article Impact Prediction with LLM</b> 
</a>
<br>
Penghai Zhao, Qinghua Xing, Kairan Dou, Jinyu Tian, <b>Y. Tai</b>, Jian Yang, Ming-Ming Cheng, Xiang Li              
<br>
<em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2025
<br>
</p>
</li>
</ul>
  

  
<!--
<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./projects/RealTalk-arxiv24.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2406.18284">
<li><p><b>RealTalk: Real-time and Realistic Audio-driven Face Generation with 3D Facial Prior-guided Identity Alignment Network</b> 
</a>
<br>
X. Ji*, C. Lin, Z. Ding, <b>Y. Tai</b>, J. Yang, J. Zhu, X. Hu, J. Zhang, D. Luo and C. Wang.              
<br>
<em>arXiv:2406.18284</em>, 2024
<br>
<a href="https://arxiv.org/pdf/2406.18284"; style="color: #EE7F2D;">arXiv</a>
<p></p>
</td><tr>
-->

<ul>
<a href="https://arxiv.org/pdf/2412.08524">
<li><p><b>Learning to Decouple the Lights for 3D Face Texture Modeling</b> 
</a>
<br>
Tianxin Huang, Zhenyu Zhang, <b>Ying Tai</b>, Gim Hee Lee.             
<br>
<em>Neural Information Processing Systems (NeurIPS)</em>, 2024
<br>
<a href="https://arxiv.org/pdf/2412.08524"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>
  
<ul>
<a href="https://arxiv.org/pdf/2405.16105">
<li><p><b>MambaLLIE: Implicit Retinex-Aware Low Light Enhancement with Global-then-Local State Space</b> 
</a>
<br>
Jiangwei Weng, Zhiqiang Yan, <b>Ying Tai</b>, Jianjun Qian, Jian Yang, Jun Li.              
<br>
<em>Neural Information Processing Systems (NeurIPS)</em>, 2024
<br>
<a href="https://arxiv.org/pdf/2405.16105"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>
  
<ul>
<a href="https://arxiv.org/pdf/2410.08192">
<li><p><b>HybridBooth: Hybrid Prompt Inversion for Efficient Subject-Driven Generation</b> 
</a>
<br>
S. Guan*, Y. Ge*, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, J. Yang, W. Li and M. You<b><span style="font-family:Wingdings">*</span></b>.              
<br>
<em>European Conference on Computer Vision (ECCV)</em>, 2024
<br>
<a href="https://arxiv.org/pdf/2410.08192"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>

<ul>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_FaceChain-ImagineID_Freely_Crafting_High-Fidelity_Diverse_Talking_Faces_from_Disentangled_Audio_CVPR_2024_paper.pdf">
<li><p><b>FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio</b> 
</a>
<br>
C. Xu, Y. Liu, J. Xing, W. Wang, M. Sun, J. Dan, T. Huang, S Li, Z. Cheng, <b>Y. Tai</b>, B. Sun
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2024
<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_FaceChain-ImagineID_Freely_Crafting_High-Fidelity_Diverse_Talking_Faces_from_Disentangled_Audio_CVPR_2024_paper.pdf"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/modelscope/facechain?tab=readme-ov-file"; style="color: #EE7F2D;">Code</a>
</p>
</li>
</ul>
  
<ul>
<a href="https://arxiv.org/pdf/2312.06354.pdf">
<li><p><b>PortraitBooth: A Versatile Portrait Model for Fast Identity-preserved Personalization</b> 
</a>
<br>
X. Peng, J. Zhu, B. Jiang, <b>Y. Tai</b>, D. Luo, J. Zhang, W. Lin, T. Jin, C. Wang, and R. Ji.              
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2024
<br>
<a href="https://arxiv.org/pdf/2312.06354.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://portraitbooth.github.io/"; style="color: #EE7F2D;">Website</a>
</p>
</li>
</ul>
  
<ul>
<a href="https://arxiv.org/pdf/2309.03508.pdf">
<li><p><b>Dynamic Frame Interpolation in Wavelet Domain</b> 
</a>
<br>
L. Kong, B. Jiang, D. Luo, W. Chu, <b>Y. Tai</b>, C. Wang, and J. Yang.              
<br>
<em>IEEE Trans. on Image Processing</em>, 2023
<br>
<a href="https://arxiv.org/pdf/2309.03508.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/ltkong218/WaveletVFI"; style="color: #EE7F2D;">Evaluation code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ltkong218/WaveletVFI.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>
  
<ul>
<a href="https://arxiv.org/pdf/2203.08612.pdf">
<li><p><b>Learning Versatile 3D Shape Generation with Improved AR Models</b> 
</a>
<br>
S. Luo, X. Qian, Y. Fu, Y. Zhang, <b>Y. Tai</b>, Z. Zhang, C. Wang, and X. Xue.              
<br>
<em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2023
<br>
<a href="https://arxiv.org/pdf/2303.14700.pdf"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/1r8jcuDpYAnwRM9HOaVJr9BgvRf9L0Vks/view?usp=share_link">
<li><p><b>Learning Neural Proto-face Field for Disentangled 3D Face Modeling In the Wild</b> 
</a>
<br>
Z. Zhang, R. Chen, W. Cao, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, and C. Wang<b><span style="font-family:Wingdings">*</span></b>
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
<br>
<a href="https://drive.google.com/file/d/1r8jcuDpYAnwRM9HOaVJr9BgvRf9L0Vks/view?usp=share_link"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>
      
      
<ul>
<a href="https://drive.google.com/file/d/1mKZ8e78DtXATv8_oZaB4d-LMBrc6UeEb/view?usp=share_link">
<li><p><b>Learning to Measure the Point Cloud Reconstruction Loss in a Representation Space</b> 
</a>
<br>
T. Huang, Z. Ding, J. Zhang, <b>Y. Tai</b>, Z. Zhang, M. Chen, C. Wang, and Y. Liu
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
<br>
<a href="https://drive.google.com/file/d/1mKZ8e78DtXATv8_oZaB4d-LMBrc6UeEb/view?usp=share_link"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>


<ul>
<a href="https://arxiv.org/pdf/2305.02572.pdf">
<li><p><b>High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning</b> 
</a>
<br>
C. Xu, J. Zhang, J. Zhu, W. Chu, <b>Y. Tai</b>, C. Wang, and Y. Liu
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
<br>
<a href="https://arxiv.org/pdf/2305.02572.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2203.11624.pdf">
<li><p><b>High-resolution Iterative Feedback Network for Camouflaged Object Detection</b> 
</a>
<br>
X. Hu, S. Wang, X. Qian, H. Dai, W. Ren, D. Luo, <b>Y. Tai</b>, and L. Shao
<br>
<em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2023
<br>
<a href="https://arxiv.org/pdf/2203.11624.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
</p>
</li>
</ul>


<ul>
<a href="https://arxiv.org/pdf/2302.03406.pdf">
<li><p><b>High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets</b> 
</a>
<br>
Y. Wang, C. Lin, D. Luo, <b>Y. Tai</b>, Z. Zhang and Y. Xie
<br>
<em>AAAI Conference on Artificial Intellige (AAAI)</em>, 2023 <b style="color: red">[Oral]</b> 
<br>
<a href="https://arxiv.org/pdf/2302.03406.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
</p>
</li>
</ul>


<ul>
<a href="https://drive.google.com/file/d/1Sl0ttF1I4uuH7WpZ89hqFliIrSRnkcYT/view?usp=sharing">
<li><p><b>3QNet: 3D Point Cloud Geometry Quantization Compression Network</b> 
</a>
<br>
T. Huang, J. Zhang, J. Chen, Z. Ding, <b>Y. Tai</b>, Z. Zhang, C. Wang, and Y. Liu
<br>
<em>ACM Transactions on Graphics</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1Sl0ttF1I4uuH7WpZ89hqFliIrSRnkcYT/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/19jhYLzOsCpDsG1ntvg6rfV-4NzHPKEwW/view?usp=sharing">
<li><p><b>ColorFormer: Image Colorization via Color Memory assisted Hybrid-attention Transformer</b> 
</a>
<br>
X. Ji*, B. Jiang*, D. Luo, G. Tao, W. Chu, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, Z. Xie, and C. Wang<b><span style="font-family:Wingdings">*</span></b>
<br>
<em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/19jhYLzOsCpDsG1ntvg6rfV-4NzHPKEwW/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1-hhTHW9aQ60JJqYKIs-6tEB1xtesjG60/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/1E2VzLtjWAM9I4ZkvveSLMqXz-frf707C/view?usp=sharing">
<li><p><b>StyleFace: Towards Identity-Disentangled Face Generation on Megapixels</b> 
</a>
<br>
Y. Luo, J. Yan, J. Zhu, K. He, W. Chu,  <b>Y. Tai</b>, and C. Wang
<br>
<em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1E2VzLtjWAM9I4ZkvveSLMqXz-frf707C/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1ncWoQXQRa7Sg_dKSpbycX0x2ieHlHZJB/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>  
</p>
</li>
</ul>

<ul>
<a href="google.com">
<li><p><b>Designing One Unified Framework for High-Fidelity Face Reenactment and Swapping</b> 
</a>
<br>
C. Xu*, J.  Zhang*, Y. Han, G. Tian, X. Zeng, <b>Y. Tai</b>, Y. Wang, C. Wang, and Y. Liu
<br>
<em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1uahR40VteHhBdjRVEN_hPwN9g5r54GZ1/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1CDDtFN6txDdALMkq0Qpc2lFGstmtnntv/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
/
<a href="https://github.com/xc-csc101/UniFace"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/xc-csc101/UniFace.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2207.10315.pdf">
<li><p><b>SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer</b> 
</a>
<br>
H. Zhou, Y. Cao, W. Chu, J. Zhu, L. Tong, <b>Y. Tai</b>, and C. Wang
<br>
<em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
<br>
<a href="https://arxiv.org/pdf/2207.10315.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
/
<a href="https://drive.google.com/file/d/1p2ubWO25o9e0d0hez6hmzXahww5j_dQc/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
/
<a href="https://github.com/hrzhou2/seedformer"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/hrzhou2/seedformer.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2207.06654.pdf">
<li><p><b>Prototypical Contrast Adaptation for Domain Adaptive Semantic Segmentation</b> 
</a>
<br>
Z. Jiang, Y. Li, C. Yang, P. Gao, Y. Wang, <b>Y. Tai</b>, and C. Wang
<br>
<em>European Conference on Computer Vision  (<b>ECCV</b>)</em>, 2022
<br>
<a href="https://arxiv.org/pdf/2207.06654.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
/
<a href="https://github.com/jiangzhengkai/ProCA"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/jiangzhengkai/ProCA.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2208.13436.pdf">
<li><p><b>Joint Learning Content and Degradation Aware Feature for Blind Super-Resolution</b> 
</a>
<br>
Y. Zhou*, C. Lin*, D. Luo, Y. Liu, Mingang Chen, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, and C. Wang<b><span style="font-family:Wingdings">*</span></b>
<br>
<em>ACM International Conference on Multimedia (<b>ACM MM</b>)</em>, 2022
<br>
<a href="https://arxiv.org/pdf/2208.13436.pdf"; style="color: #EE7F2D;">Paper (arXiv)</a>
</p>
</li>
</ul>

<ul>
<a href="https://link.springer.com/chapter/10.1007/978-3-031-16446-0_38">
<li><p><b>AutoGAN-Synthesizer: Neural Architecture Searchfor Cross-Modality MRI Synthesis</b> 
</a>
<br>
X. Hu, R. Shen, D. Luo, <b>Y. Tai</b>, C. Wang, and B. Menze
<br>
<em>International Conference on Medical Image Computing and Computer Assisted Intervention (<b>MICCAI</b>)</em>, 2022
<br>
<a href="https://link.springer.com/chapter/10.1007/978-3-031-16446-0_38"; style="color: #EE7F2D;">Paper (official link)</a>
/
<a href="https://github.com/HUuxiaobin/AutoGAN-Synthesizer"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/HUuxiaobin/AutoGAN-Synthesizer.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/1ZLG3hQuXJ9IkOqvhKyMHuPvS55sJXud3/view?usp=sharing">
<li><p><b>HifiHead: One-Shot High Fidelity Neural Head Synthesis with 3D Control</b> 
</a>
<br>
F. Zhu, J. Zhu, W. Chu, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, Z. Xie, X. Huang, and C. Wang<b><span style="font-family:Wingdings">*</span></b>.        
<br>
<em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1ZLG3hQuXJ9IkOqvhKyMHuPvS55sJXud3/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/18f2Spu3Qx1gskjdGh1Edx1ZlWjlC8xGB/view?usp=sharing">
<li><p><b>Blind Face Restoration via Integrating Face Shape and Generative Priors</b> 
</a>
<br>
F. Zhu, J. Zhu, W. Chu, X. Zhang, X. Ji, C. Wang<b><span style="font-family:Wingdings">*</span></b>, and <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>.        
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/18f2Spu3Qx1gskjdGh1Edx1ZlWjlC8xGB/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1BhJa5I95sAzENzo0_He-L8vcsYFlKL9N/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
/
<a href="https://github.com/TencentYoutuResearch/FaceRestoration-sgpn"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/FaceRestoration-sgpn.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/1oj4uCqu7E2913JKLohu7nZsj5SvECCdq/view?usp=sharing">
<li><p><b>IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation</b> 
</a>
<br>
L. Kong*, B. Jiang*, D. Luo, W. Chu, X. Huang, <b>Y. Tai</b>, C. Wang, and J. Yang.        
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1oj4uCqu7E2913JKLohu7nZsj5SvECCdq/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/15K_m9HhGQ4MfrUREr0DxcdtEuCnv_OD3/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>    
/
<a href="https://github.com/ltkong218/IFRNet"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ltkong218/IFRNet.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/1s-6glim2XEKNGmrRJKrXyKFJ1X58B7GA/view?usp=sharing">
<li><p><b>Physically-guided Disentangled Implicit Rendering for 3D Face Modeling</b> 
</a>
<br>
Z. Zhang, Y. Ge, <b>Y. Tai</b>, W. Cao, R. Chen, K. Liu, H. Tang, X. Huang, C. Wang, Z. Xie, and D. Huang.        
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1s-6glim2XEKNGmrRJKrXyKFJ1X58B7GA/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1UohmPBmgoo6-Dh7ES2ugLvy9-X2TVDNi/view?usp=sharing"; style="color: #EE7F2D;">Supp</a> 
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/14A4zxu6u5vQ9kROA4HvWllRQ72P8Ml6A/view?usp=sharing">
<li><p><b>Learning to Restore 3D Face from In-the-Wild Degraded Images</b> 
</a>
<br>
Z. Zhang, Y. Ge, <b>Y. Tai</b>, X. Huang, C. Wang, H. Tang, D. Huang, and Z. Xie.        
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/14A4zxu6u5vQ9kROA4HvWllRQ72P8Ml6A/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1ahwfZtPB4YrLWXswsDz2jSPGon2ZCiZa/view?usp=sharing"; style="color: #EE7F2D;">Supp</a>
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/1Gq-167f2ue30463K4XMkHOJThg9Yugk0/view?usp=sharing">
<li><p><b>Learning to Memorize Feature Hallucination for One-Shot Image Generation</b> 
</a>
<br>
Y. Xie, Y. Fu, J. Zhu, <b>Y. Tai</b>, Y. Cao, and C. Wang.        
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1Gq-167f2ue30463K4XMkHOJThg9Yugk0/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/1dlX3bHVJ8-djyDeuK3B-EhJoNEwxQMSY/view?usp=sharing">
<li><p><b>DIRL: Domain-invariant Representation Learning for Generalizable Semantic Segmentation</b> 
</a>
<br>
Q. Xu, L. Yao, Z. Jiang, G. Jiang, W. Chu, W. Han, W. Zhang, C. Wang, and <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>.        
<br>
<em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022 <b style="color: red">[Oral]</b> 
<br>
<a href="https://drive.google.com/file/d/1dlX3bHVJ8-djyDeuK3B-EhJoNEwxQMSY/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/1LOo18uoqCBZ_fb3HJYOcBVmUMBEVCZRj/view?usp=sharing">
<li><p><b>SCSNet: Simultaneously Image Colorization and Super-Resolution</b> 
</a>
<br>
J. Zhang, C. Xu, Y. Han, J. Li, Y. Wang, <b>Y. Tai</b>, C. Wang, F. Huang, Z. Xie, and Y. Liu.      
<br>
<em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1LOo18uoqCBZ_fb3HJYOcBVmUMBEVCZRj/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/1aCz2WOuxa3ppYkb2i43i6_WamA9drLU9/view?usp=sharing">
<li><p><b>LCTR: On Awakening the Local Continuity of Transformer for Weakly Supervised Object Localization</b> 
</a>
<br>
Z. Chen, C. Wang, Y. Wang, G. Jiang, Y. Shen, <b>Y. Tai</b>, C. Wang, W. Zhang, and L. Cao.       
<br>
<em>AAAI Conference on Artificial Intellige (<b>AAAI</b>)</em>, 2022
<br>
<a href="https://drive.google.com/file/d/1aCz2WOuxa3ppYkb2i43i6_WamA9drLU9/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2110.12151.pdf">
<li><p><b>Spectrum-to-Kernel Translation for Accurate Blind Image Super-Resolution</b> 
</a>
<br>
G. Tao, X. Ji, W. Wang, S. Chen, C. Lin, Y. Cao, T. Lu, D. Luo, and <b>Y. Tai</b>.             
<br>
<em>Thirty-fifth Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/pdf/2110.12151.pdf"; style="color: #EE7F2D;">arXiv</a>
<p><font color="red">A novel framework S2K that predicts the kernel from spectrum in frequency domain</font></p>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2105.15089.pdf">
<li><p><b>Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model</b> 
</a>
<br>
J. Zhang, C. Xu, J. Li, W. Chen, Y. Wang, <b>Y. Tai</b>, S. Chen, C. Wang, F. Huang and R. Liu.             
<br>
<em>Thirty-fifth Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/pdf/2105.15089.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/TencentYoutuResearch/BaseArchitecture-EAT"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/BaseArchitecture-EAT.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2107.12746.pdf">
<li><p><b>Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework</b> 
</a>
<br>
Q. Song*, C. Wang*, Z. Jiang, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Wu.              
<br>
<em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2021 <b style="color: red">[Oral]</b> 
<br>
<a href="https://arxiv.org/pdf/2107.12746.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-P2PNet.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2107.12619.pdf">
<li><p><b>Uniformity in Heterogeneity: Diving Deep into Count Interval Partition for Crowd Counting</b> 
</a>
<br>
C. Wang*, Q. Song*, B. Zhang, Y. Wang, <b>Y. Tai</b>, X. Hu, C. Wang, J. Li, J. Ma, and Y. Wu.             
<br>
<em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/pdf/2107.12619.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-UEPNet.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>


<ul>
<a href="https://arxiv.org/pdf/2003.11228.pdf">
<li><p><b>ASFD: Automatic and Scalable Face Detector</b> 
</a>
<br>
J. Li*, B. Zhang*, Y. Wang, <b>Y. Tai</b>, Z. Zhang, C. Wang, J. Li, X. Huang and Y. Xia.              
<br>
<em>ACM International Conference on Multimedia (<b>ACM MM</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;">arXiv</a>
<p><font color="red">Ranked No. 1 on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a></font></p>
</p>
</li>
</ul>


<ul>
<a href="https://arxiv.org/pdf/2106.09965.pdf">
<li><p><b>HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping</b> 
</a>
<br>
Y. Wang*, X. Chen*, J. Zhu, W. Chu, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, C. Wang, J. Li, Y. Wu, F. Huang and R. Ji.              
<br>
<em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
<br>
<a href="https://arxiv.org/pdf/2106.09965.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://johann.wang/HifiFace/"; style="color: #EE7F2D;">Project</a>
/
<a href="https://drive.google.com/file/d/1K9QsPX2Yw7iUtH33aoP-dTlQhnZLQuWC/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>  
/
<a href="https://drive.google.com/file/d/19i-4tJD7NxrqtmlRUl9l3M-FuMy7F2EG/view?usp=sharing"; style="color: #EE7F2D;">Video (1min)</a>  
</p>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2106.07220.pdf">
<li><p><b>Context-Aware Image Inpainting with Learned Semantic Priors</b> 
</a>
<br>
W. Zhang, J. Zhu, <b>Y. Tai</b>, Y. Wang, W. Chu, B. Ni, C. Wang and X. Yang.              
<br>
<em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
<br>
<a href="https://arxiv.org/pdf/2106.07220.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://arxiv.org/pdf/2112.04107.pdf"; style="color: #EE7F2D;">Extended journal version</a>
/
<a href="https://github.com/WendongZh/SPL"; style="color: #EE7F2D;">Code (Official)</a>    
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/WendongZh/SPL.svg" alt="GitHub stars" title="" />
</p>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2106.16128.pdf">
<li><p><b>Dual Reweighting Domain Generalization for Face Presentation Attack Detection</b> 
</a>
<br>
S. Liu, K. Zhang, T. Yao, K. Sheng, S. Ding, <b>Y. Tai</b>, J. Li, Y. Xie and L. Ma.              
<br>
<em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
<br>
<a href="https://arxiv.org/pdf/2106.16128.pdf"; style="color: #EE7F2D;">arXiv</a>
</p>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2105.11237.pdf">
<li><p><b>SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking</b> 
</a>
<br>
J. Peng*, Z. Jiang*, Y. Gu*, Y. Wu, Y. Wang, <b>Y. Tai</b>, C. Wang and W. Lin.              
<br>
<em>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>)</em>, 2021
<br>
<a href="https://arxiv.org/pdf/2105.11237.pdf"; style="color: #EE7F2D;">arXiv</a>
</p>
</p>
</li>
</ul>

<ul>
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Aggregate_and_Personalize_3D_Face_From_In-the-Wild_Photo_CVPR_2021_paper.pdf">
<li><p><b>Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection</b> 
</a>
<br>
Z. Zhang, Y. Ge, R. Chen, <b>Y. Tai</b>, Y. Yan, J. Yang, C. Wang, J. Li, and F. Huang.              
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 <b style="color: red">[Oral]</b> 
<br>
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Aggregate_and_Personalize_3D_Face_From_In-the-Wild_Photo_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/TencentYoutuResearch/3DFaceReconstruction-LAP"; style="color: #EE7F2D;">Code (Official)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/3DFaceReconstruction-LAP.svg" alt="GitHub stars" title="" />
</p>
</p>
</li>
</ul> 


<ul>
<a href="https://arxiv.org/abs/2103.13137">
<li><p><b>Learning Salient Boundary Feature for Anchor-free Temporal Action Localization</b> 
</a>
<br>
C. Lin*, C. Xu*, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.              
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/abs/2103.13137"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Learning_Salient_Boundary_Feature_for_Anchor-free_Temporal_Action_Localization_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/TencentYoutuResearch/ActionDetection-AFSD"; style="color: #EE7F2D;">Code (Official)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/ActionDetection-AFSD.svg" alt="GitHub stars" title="" />
</p>
</p>
</li>
</ul>

<ul>
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Restore_Hazy_Video_A_New_Real-World_Dataset_and_CVPR_2021_paper.pdf"><li><p><b>Learning to Restore Hazy Video: A New Real-World Dataset and A New Method</b> </a><br>
X. Zhang*, H. Dong*, J. Pan, C. Zhu, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and F. Wang.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021 
<br>
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Restore_Hazy_Video_A_New_Real-World_Dataset_and_CVPR_2021_paper.pdf"; style="color: #EE7F2D;">Paper</a>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2012.10102.pdf"><li><p><b>Frequency Consistent Adaptation for Real World Super Resolution</b> </a><br>
X. Ji*, G. Tao*, Y. Cao, <b>Y. Tai</b>, T. Lu, C. Wang, J. Li, and F. Huang.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021 
<br>
<a href="https://arxiv.org/pdf/2012.10102.pdf"; style="color: #EE7F2D;">arXiv</a>
</p>
<p><font color="red">Improved version of our prior work RealSR</font></p>
</p>
</li>
</ul>    

<ul>
<a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"><li><p><b>Learning Comprehensive Motion Representation for Action Recognition</b> </a><br>
M. Wu*, B. Jiang*, D. Luo, J. Yan, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, and X. Yang.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
<br>
<a href="https://drive.google.com/file/d/1ymzUV5HVsulyE-ADPPndZ-_E95WSfuln/view?usp=sharing"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/TencentYoutuResearch/ActionRecognition-CMR"; style="color: #EE7F2D;">Code (Official)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/ActionRecognition-CMR.svg" alt="GitHub stars" title="" />
</p>
<p><font color="red">Extented version of our prior works TEINet and TDRL</font></p>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2105.02453.pdf"><li><p><b>Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing</b> </a><br>
Z. Chen, T. Yao, K. Sheng, S. Ding, <b>Y. Tai</b>, J. Li, F. Huang, and X. Jin.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
<br>
<a href="https://arxiv.org/pdf/2105.02453.pdf"; style="color: #EE7F2D;">arXiv</a>
</p>
</li>
</ul>

<ul>
<a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"><li><p><b>To Choose or to Fuse? Scale Selection for Crowd Counting</b> </a><br>
Q. Song*, C. Wang*, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, J. Wu, and J. Ma.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2021  
<br>
<a href="https://drive.google.com/file/d/1YTcir2vUZ2zza39RSqq0wDNtt3oC90Pt/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/TencentYoutuResearch/CrowdCounting-SASNet"; style="color: #EE7F2D;">Code (Official)</a> 
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/TencentYoutuResearch/CrowdCounting-SASNet" alt="GitHub stars" title="" />
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/1911.11680.pdf"><li><p><b>FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization</b> </a><br>
X. Yin, <b>Y. Tai</b>, Y. Huang and X. Liu.
<br>
<em>Asian Conference on Computer Vision (<b>ACCV</b>)</em>, 2020
<br>
<a href="https://arxiv.org/pdf/1911.11680.pdf"; style="color: #EE7F2D;">Paper</a>
</p>
<p><font color="red">Novel framework to improve surveillance face recognition & normalization from unpaired data</font></p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2002.03662.pdf"><li><p><b>Improving Face Recognition from Hard Samples via Distribution Distillation Loss</b> </a><br>
Y. Huang*, P. Shen*, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, S. Li<b><span style="font-family:Wingdings">*</span></b>, X. Liu, J. Li, F. Huang, and R. Ji.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
<br>
<a href="https://arxiv.org/pdf/2002.03662.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750137.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/Tencent/TFace"; style="color: #EE7F2D;">Code (Official)</a> 
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/TFace.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>      

<ul>
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"><li><p><b>SSCGAN: Facial Attribute Editing via Style Skip Connections</b> </a><br>
W. Chu, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, C. Wang, J. Li, F. Huang, and R. Ji.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
<br>
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600409.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
</p>
</li>
</ul>            


<ul>
<a href="https://arxiv.org/pdf/2008.08250.pdf"><li><p><b>Face Anti-Spoofing via Disentangled Representation Learning</b> </a><br>
K. Zhang, T. Yao, J. Zhang, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, S. Ding, J. Li, F. Huang, H. Song and L. Ma.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
<br>
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640630.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2007.14557.pdf"><li><p><b>Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End <br> Joint Multiple-Object Detection and Tracking</b> </a><br>
J. Peng, C. Wang, F. Wan, Y. Wu, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and Y. Fu.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020 <b style="color: red">[Spotlight]</b> 
<br>
<a href="https://arxiv.org/pdf/2007.14557.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490137.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/pjl1995/CTracker"; style="color: #EE7F2D;">Code (Official)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/pjl1995/CTracker.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>    

<ul>
<a href="https://arxiv.org/pdf/2007.07626.pdf"><li><p><b>Temporal Distinct Representation Learning for 2D-CNN-based Action Recognition</b> </a><br>
J. Weng, D. Luo, Y. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, X. Jiang and J. Yuan.        <br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
<br>
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520358.pdf"; style="color: #EE7F2D;">Paper (Official)</a>
</p>
</li>
</ul>

<ul>
<a href="https://arxiv.org/pdf/2008.00697.pdf"><li><p><b>Adversarial Semantic Data Augmentation for Human Pose Estimation</b> </a><br>
Y. Bin, X. Cao, X. Chen, Y. Ge, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang, C. Gao and N. Sang.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
<br>
<a href="https://arxiv.org/pdf/2008.00697.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640596.pdf"; style="color: #EE7F2D;">Paper (Official)</a>  
/
<a href="https://github.com/Binyr/ASDA"; style="color: #EE7F2D;">Code (Official)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Binyr/ASDA.svg" alt="GitHub stars" title="" />

</p>
<p><font color="red">State-of-the-art performance on <a href="http://human-pose.mpi-inf.mpg.de/#results"; style="color: #EE7F2D;"> <b>MPII</b></a></font> and <a href="http://sam.johnson.io/research/lsp.html"; style="color: #EE7F2D;"> <b>LSP</b></a></font></p>
</p>
</li>
</ul>      


<ul>
<a href="https://arxiv.org/pdf/2005.01996.pdf"><li><p><b>Real-World Super-Resolution via Kernel Estimation and Noise Injection</b> </a><br>
X. Ji, Y. Cao, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, C. Wang, J. Li, and F. Huang.
<br>
<em>Computer Vision and Pattern Recognition Workshop (<b>CVPRW</b>)</em>, 2020
<br>
<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/Tencent/Real-SR"; style="color: #EE7F2D;">Code (Tencent)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/Real-SR.svg" alt="GitHub stars" title="" />
/
<a href="https://github.com/Tencent/Real-SR"; style="color: #EE7F2D;">Code (Personal)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/jixiaozhong/RealSR.svg" alt="GitHub stars" title="" />
/
<a href="https://github.com/nihui/realsr-ncnn-vulkan"; style="color: #EE7F2D;">Code (NCNN-vulkan)</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/nihui/realsr-ncnn-vulkan.svg" alt="GitHub stars" title="" />
</p>
<p><font color="red">Winner of <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a></font></p>
</p>
</li>
</ul>     

<ul>
<a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"><li><p><b>CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition</b> </a><br>
Y. Huang, Y. Wang, <b>Y. Tai</b><b><span style="font-family:Wingdings">*</span></b>, X. Liu, P. Shen, S. Li<b><span style="font-family:Wingdings">*</span></b>, J. Li, and F. Huang.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
<br>
<a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/HuangYG123/CurricularFace"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/HuangYG123/CurricularFace.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>     

<ul>
<a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"><li><p><b>Learning by Analogy: Reliable Supervision from Transformations for <br> Unsupervised Optical Flow Estimation</b> </a><br>
L. Liu, J. Zhang, Y. Liu, Y. Wang, <b>Y. Tai</b>, D. Luo, C. Wang, J. Li, and F. Huang.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
<br>
<a href="https://drive.google.com/file/d/1b_M7RQQTb2fG67MjgXZsV8CTQNaAcCI6/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/lliuz/ARFlow"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/lliuz/ARFlow.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul> 

<ul>
<a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"><li><p><b>Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification</b> </a><br>
Y. Yan, J. Qin, J. Chen, L. Liu, F. Zhu, <b>Y. Tai</b>, and L. Shao.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
<br>
<a href="https://drive.google.com/file/d/1qW8AUV2ErVQq6upTKozk7OyWMcSR5Bva/view?usp=sharing"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/daodaofr/hypergraph_reid"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/daodaofr/hypergraph_reid.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>


<ul>
<a href="https://arxiv.org/pdf/1911.04127.pdf"><li><p><b>Fast Learning of Temporal Action Proposal via Dense Boundary Generator</b> </a><br>
C. Lin*, J. Li*, Y. Wang, <b>Y. Tai</b>, D. Luo, Z. Cui, C. Wang, J. Li, F. Huang and R. Ji.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2020
<br>
<a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/Tencent/ActionDetection-DBG"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/ActionDetection-DBG.svg" alt="GitHub stars" title="" />

<p><font color="red">Ranked No. 1 on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a></font></p> 
</p>
</li>
</ul>        

<ul>
<a href="https://arxiv.org/pdf/1911.09435.pdf"><li><p><b>TEINet: Towards an Efficient Architecture for Video Recognition</b> </a><br>
Z. Liu*, D. Luo*, Y. Wang, L. Wang, <b>Y. Tai</b>, C. Wang, J. Li, F. Huang and T. Lu.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2020
<br>
<a href="https://arxiv.org/pdf/1911.09435.pdf"; style="color: #EE7F2D;">arXiv</a>
<br>      
</p>
</li>
</ul> 


<ul>
<a href="https://128.84.21.199/abs/1810.10220"><li><p><b>DSFD: Dual Shot Face Detector</b> </a><br>
J. Li, Y. Wang, C. Wang, <b>Y. Tai</b>, J. Qian, J. Yang, C.e Wang, J. Li and F. Huang.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2019
<br>
<a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://drive.google.com/open?id=1DuhcD4Thwv8E3kcLZIOQTeo8qaGDq9PB"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://github.com/Tencent/FaceDetection-DSFD"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/Tencent/FaceDetection-DSFD.svg" alt="GitHub stars" title="" />

<p><font color="red">Ranked No. 1 on <a href="http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html"; style="color: #EE7F2D;"> <b>WIDER FACE</b></a> and <a href="http://vis-www.cs.umass.edu/fddb/results.html#eval"; style="color: #EE7F2D;"> <b>FDDB</b></a> (Until 2019.01)</font></p> 
</p>
</li>
</ul>


<ul>
<a href="https://arxiv.org/pdf/1811.00342.pdf"><li><p><b>Towards Highly Accurate and Stable Face Alignment for High-Resolution Videos</b> </a><br>
<b>Y. Tai*</b>, Y. Liang*, X. Liu, L. Duan, J. Li, C. Wang, F. Huang and Y. Chen.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2019
<br>
<a href="https://arxiv.org/pdf/1811.00342.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://drive.google.com/open?id=1bDxP_i2ETfoO4uAbPhukCcxvOrVQuwd-"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://drive.google.com/open?id=1cFyjZWdGOBZ8t-63bZehERMaKpTkawwe"; style="color: #EE7F2D;">Supp</a>
/
<a href="https://github.com/tyshiwo/FHR_alignment"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/FHR_alignment.svg" alt="GitHub stars" title="" />
</p>
</li>
</ul>  


<ul>
<a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"><li><p><b>Data-Adaptive Metric Learning with Scale Alignment</b> </a><br>
S. Chen, C. Gong, J. Yang, <b>Y. Tai</b>, L. Hui and J. Li.
<br>
<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2019
<br>
<a href="https://drive.google.com/open?id=1Vm5K9OkWG7tZ4bPT3wLFDVijklEm4i1s"; style="color: #EE7F2D;">Paper (Official)</a>
<br>
</p>
</li>
</ul>         


<ul>
<a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"><li><p><b>Person Search via A Mask-Guided Two-Stream CNN Model</b> </a><br>
D. Chen, S. Zhang, W. Ouyang, J. Yang and <b>Y. Tai</b>.
<br>
<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2018
<br>
<a href="https://drive.google.com/file/d/16MaYmUcVdvYJpj83mxV4SrvpXXYYzICd/view?usp=sharing"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://drive.google.com/file/d/1Wg4ieK4BQc5XMZCBPpQJXumoHs21AHBM/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
<br>
</p>
</li>
</ul>


<ul>
<a href="https://arxiv.org/pdf/1711.10703.pdf"><li><p><b>FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors</b> </a><br>
<b>Y. Tai*</b>, Y. Chen*, X. Liu, C. Shen, J. Yang.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2018 <b style="color: red">[Spotlight]</b>
<br>
<a href="https://drive.google.com/file/d/10i2NZfUyf2Yold4ABusz3Que-XN_gEEu/view"; style="color: #EE7F2D;">Paper (Official)</a>
/
<a href="https://arxiv.org/pdf/1711.10703.pdf"; style="color: #EE7F2D;">arXiv</a>
/
<a href="https://github.com/tyshiwo/FSRNet"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/FSRNet.svg" alt="GitHub stars" title="" />
/
<a href="https://www.youtube.com/watch?v=RgHum6TB5aM"; style="color: #EE7F2D;">Demo</a>
/
<a href="https://drive.google.com/open?id=1U117O2ZyBDKVpPL06JA3g3uFtL6vDsp0"; style="color: #EE7F2D;">Slides</a>
/
<a href="https://drive.google.com/file/d/1JaSUxohFUz8m9ukAFzCbiT281nSG5lkY/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>
</p>
</li>
</ul>


<ul>
<a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf">
<li><p><b>MemNet: A Persistent Memory Network for Image Restoration</b> </a><br>
<b>Y. Tai</b>, J. Yang, X. Liu, C. Xu.
<br>
<em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2017 <b style="color: red">[Spotlight]</b>
<br>
<a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/tyshiwo/MemNet"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/MemNet.svg" alt="GitHub stars" title="" />
/
<a href="https://drive.google.com/open?id=1maokCHq_EzOzft8UtaAFOOUzpgE6ehuk"; style="color: #EE7F2D;">Poster</a>
/
<a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/"; style="color: #EE7F2D;"> <b>'15 most influential papers' in ICCV 2017 by PaperDigest</b></a>
</p>
<p><font color="red">My second paper that achieves over 1,500 google scholar citations</font></p> 
</p>
</li>
</ul>        

<ul>
<a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"><li><p><b>Image Super-Resolution via Deep Recursive Residual Network</b> </a><br>
<b>Y. Tai</b>, J. Yang, X. Liu.
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2017 
<br>
<a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"; style="color: #EE7F2D;">Paper</a>
/
<a href="https://github.com/tyshiwo/DRRN_CVPR17"; style="color: #EE7F2D;">Code</a>
<img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tyshiwo/DRRN_CVPR17.svg" alt="GitHub stars" title="" />
/
<a href="http://cvlab.cse.msu.edu/project-super-resolution.html"; style="color: #EE7F2D;">Project</a>
/
<a href="https://drive.google.com/file/d/1rWtZL3rQK5uzlw5lyPabop7MrbE0KS7j/view?usp=sharing"; style="color: #EE7F2D;">Poster</a>

</p>
<p><font color="red">My first paper that achieves over 2,000 google scholar citations</font></p> 
</p>
</li>
</ul> 


<ul>
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"><li><p><b>Nuclear Norm based Matrix Regression with Applications to Face Recognition with Occlusion and Illumination Changes</b> </a><br>
J. Yang, L. Luo, J. Qian, <b>Y. Tai</b>, F. Zhang and Y. Xu.
<br>
<em>IEEE Trans. on Pattern Analysis and Machine Intelligence</em>, 2017
<br>
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420697"; style="color: #EE7F2D;">Paper</a>
<br>      
</p>
</li>
</ul>


<ul>
<a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"><li><p><b>Structural Orthogonal Procrustes Regression for Face Recognition with Pose Variations and Misalignment</b> </a><br>
<b>Y. Tai</b>, J. Yang, F. Zhang, Y. Zhang, L. Luo, J. Qian.
<br>
<em>SIAM Conference on Data Mining (<b>SDM</b>)</em>, 2016 <b style="color: red">[Oral]</b> 
<br>
<a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.78"; style="color: #EE7F2D;">Paper</a>
<br>      
</p>
</li>
</ul>   

<ul>
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"><li><p><b>Face Recognition with Pose Variations and Misalignment via Orthogonal Procrustes Regression</b> </a><br>
<b>Y. Tai</b>, J. Yang, Y. Zhang, L. Luo, J. Qian and Y. Chen
<br>
<em>IEEE Trans. on Image Processing</em>, 2016
<br>
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448432"; style="color: #EE7F2D;">Paper</a>
<br>      
</p>
</li>
</ul>   

<ul>
<a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"><li><p><b>Learning Discriminative Singular Value Decomposition Representation for Face Recognition</b> </a><br>
<b>Y. Tai</b>, J. Yang, L. Luo, F. Zhang and J. Qian
<br>
<em>Pattern Recognition</em>, 2016
<br>
<a href="http://www.sciencedirect.com/science/article/pii/S0031320315002988"; style="color: #EE7F2D;">Paper</a>
<br>  
</p>
</li>
</ul> 


</td>
</tr>
</table>
</body>
</html>
