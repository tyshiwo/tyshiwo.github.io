<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="nju.css" type="text/css" />
<title>Ying Tai's Homepage</title>
</head>
<body>
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f85ae40b6d54f8f7becb3b0be41d4515";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="image-container">
    <img src="./projects/1200px-NJU.svg.png" width="90px" height="112px" alt="NJU">
</div>
<div class="menu-item"><a href="index.html" class="current">Homepage</a></div>
<div class="menu-item"><a href="pub.html">Publications</a></div>
<div class="menu-item"><a href="group.html">Members</a></div>
<div class="menu-item"><a href="service.html">Services</a></div>
<div class="menu-item"><a href="award.html">Awards</a></div>
<!-- <div class="menu-item"><a href="join.html">Join&nbsp;us</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Ying Tai's Homepage</h1>
</div>
<table class="imgtable"><tr><td>
<img src="./projects/yingtai.JPG" alt="Ying Tai's profile picture" width="400px" height="280px" />&nbsp;</td>
<td align="left"><p><b><a href="https://njusz.nju.edu.cn/b0/7a/c52366a635002/page.htm">Dr. Ying Tai (邰颖)</a></b>
</p>
<p><b>Associate Professor (PhD Advisor)</b>
</p>
<p><a href="https://njusz.nju.edu.cn/main.htm" target=&ldquo;blank&rdquo;>Nanjing University (Suzhou Campus)</a>
</p>
<p>1520 Taihu Road, Suzhou, P.R. China
</p>
<p><b>Email:</b> yingtai(at)nju.edu.cn; tyshiwo(at)gmail.com
</p>
<p><a href="https://scholar.google.com/citations?user=NKaiUasAAAAJ&hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a> | 
  <a href="https://github.com/tyshiwo/" target=&ldquo;blank&rdquo;>Github</a> / <a href="https://github.com/NJU-PCALab/" target=&ldquo;blank&rdquo;>Github (Group)</a> |
  <a href="https://www.scopus.com/authid/detail.uri?authorId=56437886200">Scopus</a>
</p>
<p><b>Public office hour:</b> For undergraduate students in NJU, feel free to drop by my office at Room 522, Nanyong Building (West) every Wednesday from 10am to 11am. 
</p>
</td></tr></table>

<div class="infoblock">
<div class="blocktitle"></div>
<div class="blockcontent">
<!-- <p><b><font color=red>Looking for self-motivated graduate students (both Ph.D. and master) working with me. For prospective students, please send me your resume and transcript. </font></b></p> -->
<p><b><font color=red><span lang="zh-CN" xml:lang="zh-CN"> Opening positions <b><a href="https://drive.google.com/file/d/1pdPaJb0xdQlhvtPqE-5M1CT7HQeL3MTx/view?usp=sharing">(RA招生简介)</a></b>: 招科研助理（RA）岗位1名，多模态视觉生成方向。需要本科/硕士应届毕业，有一定科研经历优先，感兴趣的同学欢迎邮件联系我。</span>
</font></b></p>
<!-- <p><b><font color=red><span lang="zh-CN" xml:lang="zh-CN"> 关于2025年招生: 25年考核制博士生名额已满，欢迎有顶会一作、ACM竞赛获奖的同学联系我。</span> -->
</font></b></p>
</div></div>
<h2>Biography</h2>
<p>I am currently an Associate Professor at School of Intelligence Science and Technology, Nanjing University (Suzhou Campus). 
  Previously, I was a Principal Researcher and Team Lead at Tencent Youtu Lab, where I spent more than 6 wonderful years, leading two teams developing novel vision algorithms that are applied in several products, e.g., Virtual Background feature in Tencent Meeting, High-fidelity face generation APIs in Tencent Cloud and Talking Face Generation for digital human product. Also, our team conducted cutting-edge research works that are published in top-tier AI conferences.
</p>
<p>I got my Ph.D. degree from the Department of Computer Science and Engineering, Nanjing University of Science & Technology (NUST) in 2017, and my advisor is Prof. <a href="http://www.patternrecognition.cn/~jian/">Jian Yang</a>. In 2016, I spent 6 wonderful months as a visiting student at Prof. <a href="http://www.cse.msu.edu/~liuxm/">Xiaoming Liu</a>'s lab in Michigan State University. 
</p>
<p>My research interests include Frontier Generative AI research and applications based on advanced large vision and language models. Specifically, I work on </p>
<ul>
<li><p>Human-centric text-to-image editing and generation:
  <a href="google.com"; style="color: #EE7F2D;"> <b>HybridBooth (ECCV'24)</b></a>;
  <a href="https://diffusion-facex.github.io/"; style="color: #EE7F2D;"> <b>FaceX (arXiv'24)</b></a>;
  <a href="https://portraitbooth.github.io/"; style="color: #EE7F2D;"> <b>PortraitBooth (CVPR'24)</b></a>
</p></li>
<li><p>Multi-modal image/video generation:
  <a href="https://arxiv.org/pdf/2407.02371v1"; style="color: #EE7F2D;"> <b>OpenVid-1M (arXiv'24)</b></a>;
  <a href="https://arxiv.org/pdf/2303.14700.pdf"; style="color: #EE7F2D;"> <b>ImAM (ICCV'23)</b></a>;
  <a href="https://arxiv.org/pdf/2305.02572.pdf"; style="color: #EE7F2D;"> <b>EmotionalTalkingFace (CVPR'23)</b></a>
</p></li>
<li><p>High-fidelity image/video restoration:  
  <a href="https://nju-pcalab.github.io/projects/AddSR/"; style="color: #EE7F2D;"> <b>AddSR (arXiv'24)</b></a>; 
  <a href="https://arxiv.org/pdf/2302.03406.pdf"; style="color: #EE7F2D;"> <b>CRI (AAAI'23)</b></a>; 
  <a href="https://drive.google.com/file/d/19jhYLzOsCpDsG1ntvg6rfV-4NzHPKEwW/view"; style="color: #EE7F2D;"> <b>Colorformer (ECCV'22)</b></a>; 
  <a href="https://drive.google.com/file/d/1oj4uCqu7E2913JKLohu7nZsj5SvECCdq/view"; style="color: #EE7F2D;"> <b>IFRNet (CVPR'22)</b></a>; 
  <a href="https://arxiv.org/pdf/2110.12151.pdf"; style="color: #EE7F2D;"> <b>S2K (NeurIPS'22)</b></a>;
  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Restore_Hazy_Video_A_New_Real-World_Dataset_and_CVPR_2021_paper.pdf"; style="color: #EE7F2D;"> <b>VideoDehaze (CVPR'21)</b></a>;
  <a href="https://arxiv.org/pdf/2012.10102.pdf"; style="color: #EE7F2D;"> <b>FCA (AAAI'21)</b></a>;
  <a href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf"; style="color: #EE7F2D;"> <b>MemNet (ICCV'17)</b></a>;
  <a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf"; style="color: #EE7F2D;"> <b>DRRN (CVPR'17)</b></a>
</p></li>
<li><p>Virtual digital human (2D and 3D): 
  <a href="https://arxiv.org/pdf/2406.18284"; style="color: #EE7F2D;"> <b>RealTalk (arXiv'24)</b></a>;
  <a href="https://arxiv.org/pdf/2403.01901.pdf"; style="color: #EE7F2D;"> <b>FaceChain-ImagineID (CVPR'24)</b></a>;
  <a href="https://drive.google.com/file/d/1r8jcuDpYAnwRM9HOaVJr9BgvRf9L0Vks/view?pli=1"; style="color: #EE7F2D;"> <b>NPF (CVPR'23)</b></a>; 
  <a href="https://arxiv.org/pdf/2106.09965.pdf"; style="color: #EE7F2D;"> <b>SGPN (CVPR'22)</b></a>; 
  <a href="https://arxiv.org/pdf/2106.09965.pdf"; style="color: #EE7F2D;"> <b>Uniface (ECCV'22)</b></a>; 
  <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136760281.pdf"; style="color: #EE7F2D;"> <b>Styleface (ECCV'22)</b></a>; 
  <a href="https://www.ijcai.org/proceedings/2022/0244.pdf"; style="color: #EE7F2D;"> <b>HifiHead (IJCAI'22)</b></a>; 
  <a href="https://arxiv.org/pdf/2106.09965.pdf"; style="color: #EE7F2D;"> <b>HifiFace (IJCAI'21)</b></a>
</p></li>
<li><p>Image/video perception and understanding:  
  <a href="https://arxiv.org/pdf/2203.11624.pdf"; style="color: #EE7F2D;"> <b>HitNet (AAAI'23)</b></a>; 
  <a href="https://arxiv.org/pdf/2207.06654.pdf"; style="color: #EE7F2D;"> <b>ProCA (ECCV'22)</b></a>; 
  <a href="https://drive.google.com/file/d/1aCz2WOuxa3ppYkb2i43i6_WamA9drLU9/view"; style="color: #EE7F2D;"> <b>LCTR (AAAI'22)</b></a>; 
  <a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view"; style="color: #EE7F2D;"> <b>Curricularface (CVPR'20)</b></a>; 
  <a href="https://arxiv.org/pdf/2007.14557.pdf"; style="color: #EE7F2D;"> <b>ChainedTracker (ECCV'20)</b></a>; 
  <a href="https://drive.google.com/file/d/1DuhcD4Thwv8E3kcLZIOQTeo8qaGDq9PB/view"; style="color: #EE7F2D;"> <b>DSFD (CVPR'19)</b></a>
</ul>

<h2>Recent Research Projects</h2>
<table class="imgtable"><tr><td>
<img src="./projects/rag.png" alt="openvid" width="600px" height="300px" />&nbsp;</td>
<td align="left"><p><a href="google.com"; style="color: #EE7F2D;"><b>Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement</b></a>
</p>
<p>
   <!-- <b><a href="google.com"; style="color: #EE7F2D;">Website</a></b>  |  -->
   <b><a href="google.com"; style="color: #EE7F2D;">arXiv (Coming soon)</a></b>  |
   <b><a href="https://github.com/NJU-PCALab/RAG-Diffusion"; style="color: #EE7F2D;">Code (Coming soon)</a></b>  | 
</p>
<p> <b>TL;DR</b>: RAG is a tuning-free Regional-Aware text-to-image Generation framework on the top of DiT-based model (FLUX.1-dev), with two novel components, Regional Hard Binding and Regional Soft Refinement, for <font color=red><b>precise and harmonious regional control</b></font>.
</p>
<p> RAG has been demonstrated to outperform <font color=red><b>Flux-1.dev (current top t2i model), SD3 (ICML'24) and RPG (ICML'24)</b></font> in complex compositional generation, excelling in aesthetics, text-image alignment, and precise control. 
</p>
<p> <small><i>Prompt: "On the left, Einstein is painting the Mona Lisa; in the center, Elon Reeve Musk is participating in the U.S. presidential election; on the right, Trump is hosting a Tesla product launch"</i></small>
</p>
</td></tr></table>
  
<table class="imgtable"><tr><td>
<img src="./projects/openvid.jpg" alt="openvid" width="600px" height="338px" />&nbsp;</td>
<td align="left"><p><a href="https://arxiv.org/pdf/2407.02371v1"; style="color: #EE7F2D;"><b>OpenVid-1M</b></a>: A Large-Scale Dataset for High-Quality Text-to-Video Generation
</p>
<p>
   <!-- <b><a href="https://nju-pcalab.github.io/projects/openvid/"; style="color: #EE7F2D;">Website</a></b>  |  -->
   <b><a href="https://huggingface.co/datasets/nankp/OpenVid-1M"; style="color: #EE7F2D;">Dataset</a></b> |
   <b><a href="https://arxiv.org/pdf/2407.02371v1"; style="color: #EE7F2D;">arXiv</a></b>  |
   <b><a href="https://huggingface.co/papers/2407.02371"; style="color: #EE7F2D;">Daily paper in HF (#2 of the day)</a></b> | 
   <b><a href="https://github.com/NJU-PCALab/OpenVid"; style="color: #EE7F2D;">Code</a></b>  | 
   <b><a href="https://huggingface.co/datasets/nkp37/OpenVid-1M/tree/main/model_weights"; style="color: #EE7F2D;">Models</a></b>  |
   <b><a href="https://www.youtube.com/watch?v=ie8JlFptZ9o&t=24s"; style="color: #EE7F2D;">Demo (High-res)</a></b> 
</p>
<p> <b>TL;DR</b>: OpenVid-1M is a high-quality text-to-video dataset designed for research institutions to enhance <font color=red><b>video quality, featuring high aesthetics, clarity, and resolution</b></font>. It can be used for direct training or as a quality tuning complement to other video datasets. It can also be used in other video generation task (video super-resolution, frame interpolation, etc)
</p>
<p> We carefully curate 1 million high-quality video clips with expressive captions to advance text-to-video research, in which <font color=red><b> 0.4 million videos are in 1080P resolution (termed OpenVidHD-0.4M) </b></font>. 
</p>
<p> OpenVid-1M is cited, discussed or used in several recent works, including video diffusion models <b><a href="https://arxiv.org/pdf/2410.20280"; style="color: #EE7F2D;">MarDini</a></b>, <b><a href="https://github.com/rhymes-ai/Allegro"; style="color: #EE7F2D;">Allegro</a></b>, <b><a href="https://t2v-turbo-v2.github.io/"; style="color: #EE7F2D;">T2V-Turbo-V2</a></b>, <b><a href="https://pyramid-flow.github.io/"; style="color: #EE7F2D;">Pyramid Flow</a></b>; long video generation model with AR model <b><a href="https://arxiv.org/pdf/2410.20502"; style="color: #EE7F2D;">ARLON</a></b>; visual understanding and generation model <b><a href="https://arxiv.org/pdf/2409.04429"; style="color: #EE7F2D;">VILA-U</a></b>; and Frame interpolation model <b><a href="https://openreview.net/pdf?id=Lp40Z40N07"; style="color: #EE7F2D;">Framer</a></b>.
</p>
<p> OpenVid-1M dataset was downloaded over 30,000 times on Huggingface last month, placing it in the <font color=red><b>top 1%</b></font> of all video datasets (as of Nov. 2024).
</p>
</td></tr></table>
  
<h2>Past Projects on Generative AI</h2>
<ul>
  <li><p style="text-align:left">High fidelity face generation: <a href="https://cloud.tencent.com/act/pro/facefusion_video?from=20218"; style="color: #EE7F2D;"><span lang="zh-CN" xml:lang="zh-CN"><b>Video face fusion (腾讯云-视频人脸融合)</b></span></a> </p></li>
  <li><p style="text-align:left">Virtual digital human: <a href="https://cloud.tencent.com/product/ivh"; style="color: #EE7F2D;"><span lang="zh-CN" xml:lang="zh-CN"><b>AI digital human (腾讯云智能-数智人)</b></span></a> </p></li>
  <li><p style="text-align:left">Potrait segmentation: <a href="https://cloud.tencent.com/document/product/1095/53925"; style="color: #EE7F2D;"><span lang="zh-CN" xml:lang="zh-CN"><b>Virtual background in Tencent Meeting (腾讯会议-虚拟背景)</b></span></a> </p></li>
</ul>
  
<h2>News</h2>
<ul>
  <li><p style="text-align:left">09/2024 – 2 papers (3D facial texture modeling and Low light enhancement via Mamba structure) accepted by NeurIPS 2024</p></li>
  <li><p style="text-align:left">09/2024 – World's Top 2% Scientists (both Career and Single year) by Stanford University</p></li> 
  <li><p style="text-align:left">07/2024 – We released the website/dataset/codes/models/arxiv of <a href="https://nju-pcalab.github.io/projects/openvid/"; style="color: #EE7F2D;"> <b>OpenVid-1M</b></a> (a high-quality text-to-video dataset to enhance video quality, featuring high aesthetics, clarity, and resolution). </p></li> 
  <li><p style="text-align:left">07/2024 – 1 paper (Efficient Subject-driven Generation) accepted by ECCV 2024 </p></li>
  <li><p style="text-align:left">06/2024 – I will be Area Chair for <a href="https://wacv2024.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2025</b></a> </p></li>
  <li><p style="text-align:left">04/2024 – Being included in <a href="https://research.com/scientists-rankings/computer-science"; style="color: #EE7F2D;"> <b>Research.com 2023 Ranking of Best Scientists in Computer Science</b></a> (#9590 in the world, #1022 in China and #12 in NJU) </p></li> 
  <li><p style="text-align:left">04/2024 – We released the codes and pretrained models of <a href="https://nju-pcalab.github.io/projects/AddSR/"; style="color: #EE7F2D;"> <b>AddSR</b></a> (accelerating inference speed of diffusion-based model for super-resolution). </p></li> 
  <li><p style="text-align:left">03/2024 – Two papers (<a href="https://portraitbooth.github.io/"; style="color: #EE7F2D;"> <b>PortraitBooth (text to portrait generation)</b></a> and <a href="https://arxiv.org/pdf/2403.01901.pdf"; style="color: #EE7F2D;"> <b>FaceChain-ImagineID</b></a> (audio to talking face generation)) accepted by CVPR'24 (8 consecutive years since 2017 :))</p></li> 
  <li><p style="text-align:left">12/2023 – Two recent papers are released: <a href="https://portraitbooth.github.io/"; style="color: #EE7F2D;"> <b>PortraitBooth (CVPR'24)</b></a> and <a href="https://diffusion-facex.github.io/"; style="color: #EE7F2D;"> <b>FaceX (arXiv'24)</b></a> (general model for popular facial editing tasks) </p></li> 
  <li><p style="text-align:left">12/2023 – 1 paper accepted by ICASSP'24 </p></li> 
  <li><p style="text-align:left">10/2023 – 2022 World's Top 2% Scientists by Stanford University (<a href="https://jokergoooo.shinyapps.io/top2pct_scientists/_w_08464be4/#"; style="color: #EE7F2D;"> <b>Ranked 5th in Tencent</b></a>) </p></li> 
  <li><p style="text-align:left">09/2023 – 1 paper (<a href="https://arxiv.org/pdf/2309.03508.pdf"; style="color: #EE7F2D;"> <b>WaveletVFI</b></a>) accepted by IEEE Transactions on Image Processing 2023 </p></li>
  <li><p style="text-align:left">08/2023 – I joined Nanjing University (Suzhou Campus) </p></li>
  <li><p style="text-align:left">07/2023 – I will be an Associate Editor for <a href="https://www.sciencedirect.com/journal/image-and-vision-computing"; style="color: #EE7F2D;"> <b>Image and Vision Computing</b></a> </p></li>
  <li><p style="text-align:left">07/2023 – 1 paper accepted by ICCV'23 </p></li>
  <li><p style="text-align:left">05/2023 – I will be an Area Chair for <a href="https://wacv2024.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2024</b></a> </p></li>
  <li><p style="text-align:left">03/2023 – 3 papers accepted by CVPR'23 </p></li>
  <li><p style="text-align:left">11/2022 – 2 papers accepted by AAAI'23 (1 Oral and 1 Poster) </p></li>
  <li><p style="text-align:left">09/2022 – 1 paper accepted by ACM Transactions on Graphics 2022 </p></li>
  <li><p style="text-align:left">07/2022 – 5 papers accepted by ECCV'22 </p></li>
  <li><p style="text-align:left">06/2022 – Our CDSR on blind super resolution is accepted by ACM MM'22, with the acceptance rate to be <b>27.9%</b> </p></li>
  <li><p style="text-align:left">06/2022 – Our AutoGAN-Synthesizer on MRI reconstruction is accepted by MICCAI'22 </p></li>
  <li><p style="text-align:left">05/2022 – I will be Area Chairs for <a href="https://wacv2023.thecvf.com/organizers/"; style="color: #EE7F2D;"> <b>WACV 2023</b></a> and <a href="http://fg2023.ieee-biometrics.org/organizers/"; style="color: #EE7F2D;"> <b>FG 2023</b></a> </p></li>
  <li><p style="text-align:left">04/2022 – Our HifiHead on high-fidelity Neural Head Synthesis is accepted by IJCAI'22, with the acceptance rate to be <b>15%</b> </p></li>
  <li><p style="text-align:left">03/2022 – Our face recognition work <a href="https://drive.google.com/file/d/1nbmqKc7fDM3zrGFLDEghnhWw_Uu8yuQ6/view?usp=sharing"; style="color: #EE7F2D;"> <b>CurricularFace (CVPR'20)</b></a> is inlcuded in <a href="https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf"; style="color: #EE7F2D;"> <b>2022 AI index report from Stanford University</b></a> </p></li>
  <li><p style="text-align:left">03/2022 – 5 papers accepted by CVPR'22, with the acceptance rate to be <b>25.3%</b> </p></li>
  <li><p style="text-align:left">03/2022 – First Prize of Progress in Science and Technology of Jiangsu Province (4/11), <a href="http://www.jiangsu.gov.cn/module/download/downfile.jsp?classid=0&filename=d926727e803e49b788d4d7e909a5949e.pdf"; style="color: #EE7F2D;"> <b>“Image restoration and robust recognition: theory and algorithms”</b></a></p></li>
  <li><p style="text-align:left">02/2022 – I will be an Area Chair for <a href="https://eccv2022.ecva.net/organizers/"; style="color: #EE7F2D;"> <b>ECCV 2022</b></a></p></li>
  <li><p style="text-align:left">12/2021 – 3 papers accepted by AAAI'22 (1 Oral and 2 Posters), with the acceptance rate to be <b>15%</b> </p></li>
  <li><p style="text-align:left">09/2021 – 2 papers on blind SR and ViT accepted by NeurIPS'21, with the acceptance rate to be <b>26%</b> </p></li>
  <li><p style="text-align:left">07/2021 – 2 papers on crowd counting accepted by ICCV'21 (1 Oral and 1 Poster), with the acceptance rate to be <b>25.9%</b> </p></li>
  <li><p style="text-align:left">07/2021 – Our <a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;"> <b>ASFD</b></a> on face detection is accepted by ACM MM'21 </p></li>
  <li><p style="text-align:left">04/2021 – 4 papers accepted by IJCAI'21, with the acceptance rate to be <b>13.9%</b> </p></li>
  <li><p style="text-align:left">04/2021 – Our Team <b>Imagination</b> is the winner of CVPR NTIRE 2021 Challenge on Video Spatial-Temporal Super-Resolution</p></li>
  <li><p style="text-align:left">03/2021 – 3 papers accepted by CVPR'21 (1 Oral and 2 Posters), with the acceptance rate to be <b>23.7%</b> </p></li>
  <li><p style="text-align:left">12/2020 – 4 papers accepted by AAAI'21, with the acceptance rate to be <b>21%</b> </p></li>
  <li><p style="text-align:left">09/2020 – Training codes of RealSR are available in Tencent official github account <a href="https://github.com/Tencent/Real-SR" target="_blank" rel="external">[Tencent-RealSR]</a>. </p></li>
  <li><p style="text-align:left">07/2020 – 6 papers accepted by ECCV'20, with the acceptance rate to be <b>27%</b> </p></li>
  <li><p style="text-align:left">05/2020 – Our RealSR model (Team name: Impressionism) won both tracks of <a href="https://arxiv.org/pdf/2005.01996.pdf"; style="color: #EE7F2D;"><b>CVPR NTIRE 2020 Challenge on Real-World Super-Resolution</b></a> </p></li>
  <li><p style="text-align:left">02/2020 – 3 papers accepted by CVPR'20, with the acceptance rate to be <b>22.1%</b> </p></li>
  <li><p style="text-align:left">11/2019 – 2 papers (Action Proposal & Action Recognition) accepted by AAAI'20, with the acceptance rate to be <b>20.6%</b>. The code of our <a href="https://arxiv.org/pdf/1911.04127.pdf"; style="color: #EE7F2D;"> <b>DBG</b></a> is released at <a href="https://github.com/TencentYoutuResearch/ActionDetection-DBG" target="_blank" rel="external">[ActionDetection-DBG]</a>, which achieves <b>Top 1</b> performance on <a href="http://activity-net.org/challenges/2019/evaluation.html"; style="color: #EE7F2D;"> <b>ActivityNet Challenge 2019 on Temporal Action Proposals</b></a> </p></li>
  <li><p style="text-align:left">02/2019 – Our <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>DSFD</b></a> on face detection is accepted by CVPR'19, with the acceptance rate to be <b>25.2%</b> </p></li>
  <li><p style="text-align:left">11/2018 – 2 papers (face alignement & adaptive metric learning) accepted by AAAI'19, with the acceptance rate to be ONLY <b>16.2%</b> </p></li>
  <li><p style="text-align:left">10/2018 – We released a novel <a href="https://128.84.21.199/abs/1810.10220"; style="color: #EE7F2D;"> <b>Dual Shot Face Detector (DSFD) </b> </a> framework that achieves <b>Top 1</b> performance on all FIVE settings of <b>WIDER FACE (Easy/Medium/Hard)</b> and <b>FDDB (Discrete/Continuous)</b> datasets </p></li>
  <li><p style="text-align:left">07/2018 – 1 paper accepted by ECCV'18 </p></li>
  <li><p style="text-align:left">02/2018 – 1 paper accepted by CVPR'18 （SPOTLIGHT Presentation）</p></li>  
  <li><p style="text-align:left">07/2017 – 1 paper accepted by ICCV'17 (SPOTLIGHT Presentation) </p></li>
  <li><p style="text-align:left">03/2017 – 1 paper accepted by CVPR'17 </p></li>
</ul>

<br>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=faf9f9&w=300&t=tt&d=hvoHWGKZcyFl6zhd6aLhpusD9f4jQY_gzPG8UfsmW0I&co=1285d6'></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha3848gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>
<script src="script.js"></script><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<div id="footer">
<div id="footer-text">
Last modified in July 2023, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
  
</td>
</tr>
</table>
</body>
</html>
